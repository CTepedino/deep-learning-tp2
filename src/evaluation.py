"""
Evaluation Module for RAG System
Sistema de evaluaciÃ³n con mÃ©tricas RAGAS
"""

import os
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

logger = logging.getLogger(__name__)


class RAGEvaluator:
    """
    Evaluador de sistemas RAG usando RAGAS
    
    MÃ©tricas implementadas:
    - Faithfulness: Â¿La respuesta es fiel al contexto recuperado?
    - Answer Relevancy: Â¿La respuesta es relevante a la pregunta?
    - Context Precision: Â¿Los contextos recuperados son precisos?
    - Context Recall: Â¿Se recuperÃ³ todo el contexto necesario?
    """
    
    def __init__(self):
        """Inicializa el evaluador con mÃ©tricas RAGAS"""
        try:
            from datasets import Dataset
            from ragas.metrics import (
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            )
            from ragas import evaluate
            
            self.Dataset = Dataset
            self.evaluate = evaluate
            self.metrics = {
                'faithfulness': faithfulness,
                'answer_relevancy': answer_relevancy,
                'context_precision': context_precision,
                'context_recall': context_recall
            }
            
            logger.info("RAGEvaluator inicializado con mÃ©tricas RAGAS")
            
        except ImportError as e:
            logger.error(f"Error importando RAGAS: {str(e)}")
            logger.error("Instala ragas con: pip install ragas datasets")
            raise
    
    def evaluate_rag(
        self,
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: Optional[List[str]] = None,
        metrics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        EvalÃºa un sistema RAG con RAGAS
        
        Args:
            questions: Lista de preguntas
            answers: Lista de respuestas generadas por el sistema
            contexts: Lista de listas de contextos recuperados para cada pregunta
            ground_truths: Lista de respuestas correctas (opcional, para context_recall)
            metrics: Lista de nombres de mÃ©tricas a usar (None = todas)
            
        Returns:
            Diccionario con resultados de evaluaciÃ³n
        """
        # Validar longitudes
        if not (len(questions) == len(answers) == len(contexts)):
            raise ValueError("questions, answers y contexts deben tener la misma longitud")
        
        # Preparar datos para RAGAS
        data_samples = {
            'question': questions,
            'answer': answers,
            'contexts': contexts,
        }
        
        # Agregar ground_truths si estÃ¡n disponibles (necesario para context_recall)
        if ground_truths:
            if len(ground_truths) != len(questions):
                raise ValueError("ground_truths debe tener la misma longitud que questions")
            data_samples['ground_truth'] = ground_truths
        
        # Crear dataset
        dataset = self.Dataset.from_dict(data_samples)
        
        # Seleccionar mÃ©tricas
        if metrics:
            selected_metrics = [self.metrics[m] for m in metrics if m in self.metrics]
        else:
            # Si no hay ground_truths, omitir context_recall
            if ground_truths:
                selected_metrics = list(self.metrics.values())
            else:
                selected_metrics = [
                    self.metrics['faithfulness'],
                    self.metrics['answer_relevancy'],
                    self.metrics['context_precision']
                ]
        
        logger.info(f"Evaluando {len(questions)} preguntas con {len(selected_metrics)} mÃ©tricas")
        
        # Evaluar
        try:
            result = self.evaluate(
                dataset,
                metrics=selected_metrics,
                raise_exceptions=False
            )
            
            # Convertir a pandas para mejor visualizaciÃ³n
            df_results = result.to_pandas()
            
            # Calcular estadÃ­sticas agregadas
            stats = {
                'num_samples': len(questions),
                'metrics_scores': {},
                'detailed_results': df_results.to_dict('records')
            }
            
            # Calcular promedios de cada mÃ©trica
            for metric_name in self.metrics.keys():
                if metric_name in df_results.columns:
                    scores = df_results[metric_name].dropna()
                    if len(scores) > 0:
                        stats['metrics_scores'][metric_name] = {
                            'mean': float(scores.mean()),
                            'std': float(scores.std()),
                            'min': float(scores.min()),
                            'max': float(scores.max())
                        }
            
            logger.info("EvaluaciÃ³n completada exitosamente")
            return {
                'status': 'success',
                'stats': stats,
                'dataframe': df_results
            }
            
        except Exception as e:
            logger.error(f"Error durante evaluaciÃ³n: {str(e)}")
            return {
                'status': 'error',
                'message': str(e)
            }
    
    def generate_report(
        self,
        evaluation_results: Dict[str, Any],
        output_file: Optional[str] = None
    ) -> str:
        """
        Genera un reporte legible de la evaluaciÃ³n
        
        Args:
            evaluation_results: Resultados de evaluate_rag()
            output_file: Archivo donde guardar el reporte (opcional)
            
        Returns:
            String con el reporte
        """
        if evaluation_results.get('status') != 'success':
            return f"Error en evaluaciÃ³n: {evaluation_results.get('message', 'Desconocido')}"
        
        stats = evaluation_results['stats']
        
        # Construir reporte
        report_lines = [
            "=" * 70,
            "ðŸ“Š REPORTE DE EVALUACIÃ“N RAG",
            "=" * 70,
            "",
            f"ðŸ“ Muestras evaluadas: {stats['num_samples']}",
            "",
            "ðŸ“ˆ MÃ‰TRICAS AGREGADAS:",
            "-" * 70,
        ]
        
        # Agregar cada mÃ©trica
        for metric_name, scores in stats['metrics_scores'].items():
            report_lines.extend([
                f"\nðŸ”¹ {metric_name.upper().replace('_', ' ')}:",
                f"   - Promedio: {scores['mean']:.4f}",
                f"   - Desv. EstÃ¡ndar: {scores['std']:.4f}",
                f"   - MÃ­nimo: {scores['min']:.4f}",
                f"   - MÃ¡ximo: {scores['max']:.4f}"
            ])
        
        # InterpretaciÃ³n de mÃ©tricas
        report_lines.extend([
            "",
            "=" * 70,
            "ðŸ’¡ INTERPRETACIÃ“N DE MÃ‰TRICAS:",
            "=" * 70,
            "",
            "ðŸŽ¯ Faithfulness (Fidelidad):",
            "   Mide si la respuesta se basa en los hechos del contexto.",
            "   1.0 = perfectamente fiel, 0.0 = no fiel",
            "",
            "ðŸŽ¯ Answer Relevancy (Relevancia de Respuesta):",
            "   Mide cuÃ¡n relevante es la respuesta a la pregunta.",
            "   1.0 = perfectamente relevante, 0.0 = irrelevante",
            "",
            "ðŸŽ¯ Context Precision (PrecisiÃ³n de Contexto):",
            "   Mide si los contextos recuperados son precisos.",
            "   1.0 = todos los contextos son relevantes, 0.0 = ninguno relevante",
            "",
            "ðŸŽ¯ Context Recall (Exhaustividad de Contexto):",
            "   Mide si se recuperÃ³ todo el contexto necesario.",
            "   1.0 = todo recuperado, 0.0 = nada recuperado",
            "   (Requiere ground_truth)",
            "",
            "=" * 70
        ])
        
        report = "\n".join(report_lines)
        
        # Guardar si se especifica archivo
        if output_file:
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Reporte guardado en: {output_file}")
        
        return report


def create_evaluator() -> RAGEvaluator:
    """
    FunciÃ³n de conveniencia para crear un evaluador
    
    Returns:
        Instancia de RAGEvaluator
    """
    return RAGEvaluator()


if __name__ == "__main__":
    # Ejemplo de uso
    logging.basicConfig(level=logging.INFO)
    
    # Datos de ejemplo
    questions = [
        "Â¿QuÃ© es una variable aleatoria?",
        "Â¿CuÃ¡l es la fÃ³rmula de la varianza?"
    ]
    
    answers = [
        "Una variable aleatoria es una funciÃ³n que asigna valores numÃ©ricos a los resultados de un experimento aleatorio.",
        "La varianza es V(X) = E[(X - Î¼)Â²]"
    ]
    
    contexts = [
        ["Una variable aleatoria es una funciÃ³n que mapea resultados de experimentos a nÃºmeros reales."],
        ["La varianza se calcula como V(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²"]
    ]
    
    # Crear evaluador
    evaluator = create_evaluator()
    
    # Evaluar
    results = evaluator.evaluate_rag(
        questions=questions,
        answers=answers,
        contexts=contexts
    )
    
    # Generar reporte
    if results['status'] == 'success':
        report = evaluator.generate_report(results)
        print(report)

