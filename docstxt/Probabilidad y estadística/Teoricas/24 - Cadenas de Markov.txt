Proceso de Markov
Se dice que un proceso estocÃ¡stico es un proceso de Markov 
sii para todo  toda secuencia  y todo 
Es decir, sÃ³lo importa el valor mÃ¡s â€œrecienteâ€.
Hechos:
â€¢
La caminata aleatoria es un proceso de Markov.
â€¢
Asumir que se trata de un proceso de Markov simplifica mucho las 
cosas.

[DescripciÃ³n de la imagen: proceso de marketing poro]

Cadena de Markov
Habitualmente, se llama cadena de Markov a un proceso de 
Markov que tiene un espacio de parÃ¡metro discreto: T.
En nuestro caso, consideraremos tambiÃ©n un espacio de 
estados discreto: .
Hecho:
â€¢
La caminata aleatoria es una cadena de Markov.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
Una cadena de Markov  queda completamente descripta por:
1.
La distribuciÃ³n de probabilidades del estado inicial
ğ‘ğ‘—(0 )=ğ‘ƒ( ğ‘‹(0)=ğ‘’ğ‘—)
2.
Las probabilidades de transiciÃ³n entre cada par de estados
ğ‘ğ‘–ğ‘—(ğ‘›)= ğ‘ƒ( ğ‘‹(ğ‘›+1)=ğ‘’ğ‘—âˆ¨ğ‘‹(ğ‘›)=ğ‘’ğ‘–)
âˆ‘
ğ‘—
ğ‘ğ‘—(0)=1
âˆ‘
ğ‘—
ğ‘ğ‘–ğ‘—(ğ‘›)=1

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
Matriz de probabilidades de transiciÃ³n de un solo paso:
â„™(ğ‘›)=(
ğ‘11 (ğ‘›)
ğ‘12 (ğ‘›)
â‹¯
ğ‘21 (ğ‘›)
ğ‘22 (ğ‘›)
â‹¯
â‹®
â‹®
â‹±)
Las filas suman 1.
Vector de probabilidades:
âƒ—ğ‘(ğ‘›)=( ğ‘1(ğ‘›)
ğ‘2(ğ‘›)
â‹¯)
Suma 1.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol | TranscripciÃ³n de la imagen: Cadena de Markov

Vector de probabilidades:

Ppin)=Ip.(n) p.(n)

Suma 1.

Matriz de probabilidades de transiciÃ³n de un solo paso:

Paulr) Pln)

Las filas suman 1.]]

Cadena de Markov
Diagrama de estados
ğ‘’1
ğ‘’2
ğ‘’3
ğ‘12
ğ‘21
ğ‘32
ğ‘23
ğ‘13
ğ‘31
ğ‘11
ğ‘22
ğ‘33

[DescripciÃ³n de la imagen: cade de marco diagrama de marco | TranscripciÃ³n de la imagen: Cadena de Markov

Diagrama de estados

D1]]

Cadena de Markov
EcuaciÃ³n de Chapman-Kolmogorov:
ğ‘ğ‘—(ğ‘›+1)=ğ‘1(ğ‘›) ğ‘1 ğ‘—(ğ‘›)+ğ‘2 (ğ‘›) ğ‘2 ğ‘—(ğ‘›)+ğ‘3(ğ‘›)ğ‘3 ğ‘—(ğ‘›)+â€¦
Forma matricial:
âƒ—ğ‘(ğ‘›+1)=âƒ—ğ‘(ğ‘›)â„™(ğ‘›)
ğ‘ğ‘—(ğ‘›+1)=âˆ‘
ğ‘–
ğ‘ğ‘–(ğ‘›)ğ‘ğ‘–ğ‘—(ğ‘›)

[DescripciÃ³n de la imagen: una nota con la ecuaciÃ³n de la ecuaciÃ³n | TranscripciÃ³n de la imagen: Cadena de Markov

EcuaciÃ³n de Chapman-Kolmogorov:

pn+1)=p,Inp, n)+p,In|p, n)+p,In)p, Â¡ IN|+...

pj(n+1):Z pi(n)pij(n)

Forma matricial:

pln+1)=p(n)P(n)]]

Cadena de Markov
Cadenas homogÃ©neas: las probabilidades de transiciÃ³n no 
dependen del tiempo
ğ‘ğ‘–ğ‘—(ğ‘›)=ğ‘ğ‘–ğ‘—(ğ‘˜)
Chapman-Kolmogorov:
Matriz de probabilidades de transiciÃ³n de k pasos:

[DescripciÃ³n de la imagen: caden de marjo cadens horges, las probadas, tras no]

Cadena de Markov
        El vendedor viajero 
La regiÃ³n de ventas de un vendedor la componen tres 
ciudades A, B y C. Nunca vende en la misma ciudad en dÃ­as 
seguidos. Si vende en la ciudad A, entonces al dÃ­a siguiente 
vende en la ciudad B. Sin embargo, si vende en una de las 
dos ciudades B o C, entonces al dÃ­a siguiente la 
probabilidad de vender en A es el doble de la de vender en 
la restante.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
        El vendedor viajero 
Diagrama de transiciÃ³n
Matriz de probabilidades de transiciÃ³n
â„™=(
0
1
0
2/3
0
1/3
2/3
1/3
0 )

[DescripciÃ³n de la imagen: un diagrama de un Ã¡rbol con un nÃºmero de 1]

Cadena de Markov
Probabilidad a largo plazo o distribuciÃ³n estacionaria:
âƒ—ğœ‹= lim
ğ‘›â†’âˆâƒ—ğ‘(ğ‘›)
No siempre existe, pero, si existe, se corresponde con un 
autovector a izquierda correspondiente al autovalor 1 de :
âƒ—ğœ‹= lim
ğ‘›â†’âˆâƒ—ğ‘(ğ‘›)=Â¿ lim
ğ‘›â†’âˆâƒ—ğ‘(ğ‘›+1)=Â¿ lim
ğ‘›â†’âˆâƒ—ğ‘(ğ‘›) â„™=[ lim
ğ‘›â†’âˆâƒ—ğ‘(ğ‘›)] â„™Â¿Â¿
âƒ—
ğœ‹= âƒ—
ğœ‹â„™
No toda distribuciÃ³n que satisface esta ecuaciÃ³n es una 
distribuciÃ³n estacionaria.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
CondiciÃ³n suficiente para la existencia de una distribuciÃ³n 
estacionaria:
Si una cadena de Markov finita es regular, esto es, existe  tal  
tiene todos sus elementos positivos (> 0), entonces existe una 
distribuciÃ³n de probabilidad esacionaria

[DescripciÃ³n de la imagen: un bloc de notas con las palabras''y''en espaÃ±ol]

Cadena de Markov
El vendedor viajero 
La cadena es regular, en 4 pasos todos los estados estÃ¡n 
conectados. 
Existe 
una 
distribuciÃ³n 
estacionaria 
de 
probabilidades independiente de la distribuciÃ³n inicial de 
probabilidades de estados. 
â„™4=(
42/81
18 /81
21 /81
26 /81
49/81
6 /81
26 /81
48/81
7 /81 )

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
El vendedor viajero 
â„™10=(
0.41040
0.42920
0.16039
0.39306
0.46387
0.14307
0.39306
0.46385
0.14308)
â„™100=(
0.40000
0.45000
0.15000
0.40000
0.45000
0.15000
0.40000
0.45000
0.15000)

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
El vendedor viajero 
âƒ—
ğœ‹= âƒ—
ğœ‹â„™
(ğ‘
ğ‘
ğ‘)=(ğ‘
ğ‘
ğ‘)(
0
1
0
2 /3
0
1/3
2 /3
1/3
0 )
ğ‘= 2
3 ğ‘+ 2
3 ğ‘
ğ‘=ğ‘+ 1
3 ğ‘
ğ‘= 1
3 ğ‘
No son l.i.

[DescripciÃ³n de la imagen: una hoja con la ecuaciÃ³n del nÃºmero del nÃºmero del nÃºmero del nÃºmero del nÃºmero del nÃºmero de | TranscripciÃ³n de la imagen: Cadena de Markov

El vendedor viajero

No son |.I.]]

Cadena de Markov
El vendedor viajero 
âƒ—
ğœ‹= âƒ—
ğœ‹â„™
(ğ‘
ğ‘
ğ‘)=(ğ‘
ğ‘
ğ‘)(
0
1
0
2 /3
0
1/3
2 /3
1/3
0 )
ğ‘= 2
3 ğ‘+ 2
3 ğ‘
1=ğ‘+ğ‘+ğ‘
ğ‘= 1
3 ğ‘
Agregamos 
una ecuaciÃ³n

[DescripciÃ³n de la imagen: una hoja con la ecuaciÃ³n de la ecuaciÃ³n | TranscripciÃ³n de la imagen: Cadena de Markov

El vendedor viajero

2 2
aâ€”5b+5c

Agregamos
1=d+b+c una ecuaciÃ³n

1
Câ€”5b]]

Cadena de Markov
El vendedor viajero 
âƒ—
ğœ‹= âƒ—
ğœ‹â„™
âƒ—
ğœ‹=(ğ‘
ğ‘
ğ‘)=( 8
2 0
9
20
3
20)
âƒ—ğœ‹=(ğ‘
ğ‘
ğ‘)= (0.40
0.45
0.15)

[DescripciÃ³n de la imagen: una hoja con las palabras'''' y'''' en espaÃ±ol | TranscripciÃ³n de la imagen: Cadena de Markov

El vendedor viajero]]

Cadena de Markov
        El vendedor viajero 
 AquÃ­ se muestra la evoluciÃ³n temporal de la distribuciÃ³n de 
probabilidades de estados en funciÃ³n del tiempo. Se supuso 
que inicialmente el vendedor estaba en la ciudad A.

[DescripciÃ³n de la imagen: cade de marko aquise muestra dedicion de distudi]

Cadena de Markov
El bebÃ©
Un modelo muy simple de un infante tiene tres estados: 1 â€“ 
Come; 2 â€“ Hace sus necesidades; 3 â€“ Duerme 
ğ¶ğ‘œ
ğ¶ğ‘
ğ·ğ‘¢
1
1
1

[DescripciÃ³n de la imagen: un diagrama de un Ã¡rbol con los nombres de las letras]

Cadena de Markov
El bebÃ©
Un modelo muy simple de un infante tiene tres estados: 1 â€“ 
Come; 2 â€“ Hace sus necesidades; 3 â€“ Duerme 
â„™=(
0
1
0
0
0
1
1
0
0)
Â¿Existe una distribuciÃ³n estacionaria?

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
El bebÃ©
(ğ‘
ğ‘
ğ‘)=(ğ‘
ğ‘
ğ‘)(
0
1
0
0
0
1
1
0
0)
ğ‘=ğ‘
1=ğ‘+ğ‘+ğ‘
ğ‘=ğ‘
âƒ—
ğœ‹= âƒ—
ğœ‹â„™
ğ‘=ğ‘=ğ‘= 1
3

[DescripciÃ³n de la imagen: una hoja con las palabras "ebe" y "ebe" | TranscripciÃ³n de la imagen: Cadena de Markov

El bebÃ©

Â£ i iiLÃ‘ .Y â€”â€”â€”

â€” â€”]]

Cadena de Markov
El bebÃ©
Sin embargo, no existe una distribuciÃ³n estacionaria: la 
cadena es periÃ³dica
â„™4=â„™=(
0
1
0
0
0
1
1
0
0)
Â¿QuÃ© significa la distribuciÃ³n obtenida? Que, en promedio, el 
bebÃ© pasa un tercio del tiempo en cada uno de los estados.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
Vida
La probabilidad de que un individuo muera durante el prÃ³ximo 
aÃ±o es . Los estados posibles son: 1 â€“ Muerto; 2 â€“ Vivo
ğ‘€
ğ‘‰
ğ‘
1
La muerte es un estado absorbente.

[DescripciÃ³n de la imagen: un diagrama de una lÃ­nea de cÃ³digo]

Cadena de Markov
Vida
La probabilidad de que un individuo muera durante el prÃ³ximo 
aÃ±o es . Los estados posibles son: 1 â€“ Muerto; 2 â€“ Vivo
â„™=(
1
0
ğ‘
ğ‘)
â„™ğ‘›=(
1
0
1 âˆ’ğ‘ğ‘›
ğ‘ğ‘›)

[DescripciÃ³n de la imagen: caden de marco vida la probable que unviddoo]

Cadena de Markov
Vida
Si (estÃ¡ vivo inicialmente), Â¿cuÃ¡ntos aÃ±os permanecerÃ¡ en 
ese estado?
ğ‘ƒ( ğ‘=1âˆ¨ğ‘‹(0)=2)=ğ‘
Llamemos  nÃºmero de aÃ±os vivo
ğ‘ƒ( ğ‘=2âˆ¨ğ‘‹(0 )=2)=ğ‘ğ‘
ğ‘ƒ( ğ‘=3âˆ¨ğ‘‹(0 )=2)=ğ‘ğ‘2
ğ‘ƒ( ğ‘=4âˆ¨ğ‘‹(0 )=2)=ğ‘ğ‘3
â‹®
ğ‘ƒ( ğ‘=ğ‘›âˆ¨ğ‘‹(0 )=2)=ğ‘ğ‘ğ‘›âˆ’1

[DescripciÃ³n de la imagen: una sÃ¡bana con las palabras''y''en espaÃ±ol | TranscripciÃ³n de la imagen: Cadena de Markov

Vida
Si (estÃ¡ vivo inicialmente), Â¿cuÃ¡ntos aÃ±os permanecerÃ¡ en

ese estado?

Llamemos nÃºmero de aÃ±os vivo
PII =1 X
PIIN =2Vv X (
PlIIV =3v X(o
PII =4v X(O

PlIIN =nv X (o)]]

Cadena de Markov
Vida
Si (estÃ¡ vivo inicialmente), Â¿cuÃ¡ntos aÃ±os permanecerÃ¡ en 
ese estado?
ğ¸(ğ‘âˆ¨ğ‘‹(0)=2)=âˆ‘
ğ‘›= 0
âˆ
ğ‘›ğ‘ƒ( ğ‘=ğ‘›âˆ¨ğ‘‹(0)=2)
Llamemos  nÃºmero de aÃ±os vivo
ğ¸( ğ‘âˆ¨ğ‘‹(0)=2)=âˆ‘
ğ‘›= 0
âˆ
ğ‘›ğ‘ğ‘ğ‘›âˆ’1
ğ¸( ğ‘âˆ¨ğ‘‹(0)=2)=
1
1âˆ’ğ‘

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
Vida mÃ¡s compleja
Un individuo puede estar en cualquiera de tres estados: 1 â€“ 
Muerto; 2 â€“ Enfermo; 3 â€“ Sano.

[DescripciÃ³n de la imagen: un bloc de notas con las palabras 'vidas' y 'vidas']

Cadena de Markov
Vida mÃ¡s compleja
ğ‘€
ğ¸
ğ‘†
0.1
0.29
0.6
0.01
1
0.3
0.7

[DescripciÃ³n de la imagen: un diagrama de un nodo nodo nodo nodo nodo nodo nodo nodo nodo nodo nodo nodo | TranscripciÃ³n de la imagen: Cadena de Markov

Vida mÃ¡s compleja]]

Cadena de Markov
Vida mÃ¡s compleja
â„™=(
1
0
0
0.10
0.30
0.60
0.01
0.29
0.70)
Estructura:
â„™=(
ğ•€
ğŸ
ğ”½
â„š)
Definimos: 
Al igual que en el ejemplo anterior, se puede demostrar que  
valor esperado del tiempo pasado en , dado que se comenzÃ³ 
en , antes de alcanzar un estado absorbente.

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

Cadena de Markov
Vida mÃ¡s compleja
ğ”¼=(ğ•€âˆ’â„š)âˆ’1=(
25
3
50
3
145
18
175
9 )
Llamemos:
 nÃºmero de aÃ±os enfermo, 
 nÃºmero de aÃ±os sano
,  
,

[DescripciÃ³n de la imagen: un cuaderno con un nÃºmero de 3 y un nÃºmero de 3]

Andrei Andreyevich Markov
(1856-1922)

[DescripciÃ³n de la imagen: una vieja foto de un hombre con barba | TranscripciÃ³n de la imagen: yevich Markov

Andre]]

Cadena de Markov
function p = markov(P,p0,N)
% P es la matriz de transiciÃ³n 
% p0 es el vector inicial de probabilidades
% N es el vector de instantes donde se desea analizar
M = rows(P);
N = sort(N); 
n  = max(N);
p  = zeros(n+1,M);
p(1,:) = p0;
for k = 1:n
  p(k+1,:) = p(k,:)*P;
end
p = p(N,:);

[DescripciÃ³n de la imagen: caden de marko]

Cadena de Markov
function X = simulmarkov(P,x0,n)
% Genera una realizaciÃ³n de una cadena de Markov
% P es la matriz de transiciÃ³n 
% x0 es el estado inicial 
% n es la cantidad de instantes considerados
M = rows(P);
E = 1:M;
X = zeros(n,1);
X(1)=x0;
for k=1:n-1
  X(k+1) = discrete_rnd(E,P(X(k),:),1);
end

[DescripciÃ³n de la imagen: una nota con las palabras''y''en espaÃ±ol]

