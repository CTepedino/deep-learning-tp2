TRANSFORMERS
SISTEMAS DE INTELIGENCIA ARTIFICIAL - 2025

[Descripción de la imagen: el logotipo para el simposio de transformadores]

TABLA DE CONTENIDOS
01. INTRODUCCIÓN
02. ARQUITECTURA
03. ENTRENAMIENTO
04. APLICACIONES
05. INDUSTRIA
06. BIBLIOGRAFÍA
2

[Descripción de la imagen: conidos de mesa | Transcripción de la imagen: TABLA DE CONTENIDOS

O1. INTRODUCCIÓN O4. APLICACIONES

O2. ARQUITECTURA O5. INDUSTRIA

03. ENTRENAMIENTO O06. BIBLIOGRAFÍA]]

01
INTRODUCCIÓN
¿Qué es un Transformer?
3

[Descripción de la imagen: un fondo blanco y amarillo con las palabras introion]

TRANSFORMER
4
TRANSFORMERS
Paper

[Descripción de la imagen: la atención de los transformadores no es | Transcripción de la imagen: TRANSEFORMERS

Attention Is All You Need

Ashish Vaswani" Noam Shazeer* Niki Parmar” Jakob Uszkoreit*
Google Brain Google Brain Google Research Google Research
avaswaniCgoogle.com ioam0google.com mnikipOgoogle.com 1uszÓgoogle.com

Llion Jones” Aidan N. Gomez" * Lukasz Kaiser*
Google Research University of Toronto Google Brain
lliongoogle.com aidan€cs.toronto.edu lukaszkaiserOgoogle.com

IHllia Polosukhin* *
illia.polosukhinOgmail.com

Abstract

The dominant seguence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention

Paper]]

¿QUÉ ES UN 
TRANSFORMER? 
Red neuronal utilizada para transformar 
una secuencia en otra basada en un 
mecanismo de atención.
Soy un estudiante
I am a student
TRANSFORMER
5

[Descripción de la imagen: ques unificas entrer ques unificas entrer ques que que]

ARQUITECTURA  
02
¿Cuáles son las capas 
ocultas de un Transformer?
6

[Descripción de la imagen: aqueta, aquer, y aquer en español]

Soy un estudiante
I am a student
ENCODERS
DECODERS
INPUT
OUTPUT
TRANSFORMER
7

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de los códigos finales]

Soy un estudiante
I am a student
ENCODER 0
INPUT
OUTPUT
TRANSFORMER
ENCODER 1
ENCODER 2
ENCODER 3
ENCODER n
...
DECODER 0
DECODER 1
DECODER 2
DECODER 3
DECODER n
...
8

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de los diferentes tipos del sistema]

Soy un estudiante
I am a student
INPUT
OUTPUT
TRANSFORMER
ENCODER n
...
 
DECODER n
...
Feed-Forward Neural Network
Self-Attention Layer
Feed-Forward Neural Network
Self-Attention Layer
Encoder-Decoder Attention
9

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de la red]

SELF-ATTENTION LAYER 
Ayudar al Encoder a mirar otras palabras en la secuencia de input mientras analiza una 
palabra en particular.
En el Decoder solo puede mirar palabras previas en la secuencia de output.
The animal didn’t cross the street because it was tired
INPUT
¿La palabra “it” se reﬁere al animal o a la calle ?
10

[Descripción de la imagen: seation l / p - screenshoto]

EMBEDDING 
Soy un estudiante
INPUT
Soy 
un
estudiante
●
Cada token es representado en un vector de números reales de dimensión 512. 
A esto se lo llama embedding (algoritmos: GloVe o word2vec: SkipGram - CBOW)
●
El primer encoder recibe como input el embedding.  
TOKENIZATION
EMBEDDING
x0
x1
x2
11

[Descripción de la imagen: emedic a sema remedo untuo o que sem]

CÁLCULO DE SELF-ATTENTION 
1.
Por cada embedding xi , se calculan 3 vectores 
llamados  query, key y value
multiplicando los embeddings por 3 matrices 
de pesos (Attention Weights) WQ, WK y WV
12
WQ
WK
WV
qi
ki
vi
x
=
=
=
x
x
xi
xi
xi

[Descripción de la imagen: calculae de lation]

CÁLCULO DE SELF-ATTENTION 
2.
Se calcula un “score” por cada embedding que 
determina cuánta atención se le pone a ese 
embedding mientras se está procesando otro.  
Se hace el producto escalar entre q y el vector k  de 
cada embedding.
13
q0
k0
x0
x1
q1
k1
q0
k0
●
= s0
q0
k1
●
= s1

[Descripción de la imagen: un diagrama con las palabras en español y español]

CÁLCULO DE SELF-ATTENTION 
3.
Se dividen los scores por la raíz de la dimensión 
de vector k (dk)
4. 
Usando Softmax para los scores se obtienen 
probabilidades (pi). 
14
q0
k0
x0
x1
q1
k1
q0
k0
●
= s0
q0
k1
●
= s1
SoftMax(s0/√dk ) = p0
SoftMax(s1/√dk ) = p1

[Descripción de la imagen: calculae de lation | Transcripción de la imagen: CÁLCULO DE SELF-ATTENTION

3. Se dividen los scores por la raíz de la dimensión

de vector k (,)

4. . Usando Softmax para los scores se obtienen

probabilidades (p_).

X0 x1
“, [ + aE
r, [ ' E

qoo kO=SO qo. k.l =S1

softMax(s//d,) = P, SoftMax(s,//a;) =

Pi]]

CÁLCULO DE SELF-ATTENTION 
5.
Se multiplica cada vector v por lo obtenido 
en la función de Softmax.
6.
Se suman los vectores  ponderados v’ 
obteniendo el output de la capa de Self-Attention 
para cada token (en este ejemplo, para el primero)
15
q0
k0
x0
x1
q1
k1
q0
k0
●
= s0
q0
k1
●
= s1
SoftMax(s0/√dk ) = p0
SoftMax(s1/√dk ) = p1
z0
 v’0= p0 * 
v0
 v’1= p1 * v1
v’0+v’1 =

[Descripción de la imagen: un diagrama del mismo número del mismo número del mismo número del mismo número del mismo número del mismo | Transcripción de la imagen: CÁLCULO DE SELF-ATTENTION

5. Se multiplica cada vector v por lo obtenido

en la función de Softmax.

6. Se suman los vectores ponderados v'
obteniendo el output de la capa de Self-Attention

para cada token (en este ejemplo, para el primero)

Xo as

1 NEE
“, [ + aE
r, [ ' E

qoº kº=50 qoº k'| =S1

SoftMax(s//d,) = P, SoftMax(s,//d,) = p,

Vo Po- ME V-
v

o

? 9 —
vo*v17 70 ME

15]]

CÁLCULO DE SELF-ATTENTION 
WQ
WK
WV
X
Soy
un
estudiante
Q
K
V
x
=
=
=
X
X
x
x
16
Attention(K,Q,V) = SoftMax( QK’ / √dk ) * V = Z  
Para implementarlo se calcula de forma 
matricial para un procesamiento más rápido

[Descripción de la imagen: un diagrama del mismo número de los mismos números]

Feed-Forward 
Neural Network
Self-Attention Layer
Soy
un
estudiante
ENCODER 0
●
Cada token sigue su propio camino en el encoder. 
●
Hay dependencias entre los caminos en la self-attention layer pero no en la red feed-forward. 
●
Los caminos se pueden ejecutar en paralelo, pasando por redes feed-forward idénticas 
Feed-Forward 
Neural Network
z2
Feed-Forward 
Neural Network
z0
z1
17

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de la red]

MULTI-HEADED ATTENTION 
Mecanismo que mejora la performance de la capa de self-attention: 
●
El Encoder se puede enfocar en distintas posiciones de la secuencia simultáneamente 
●
Se tienen múltiples sets K Q V, llamados “heads”
18
K0
Q0
V0
K1
Q1
V1
Kn
Qn
Vn
HEAD 0
HEAD 1
HEAD n
...

[Descripción de la imagen: methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - methodee - - - - - - - - - -]

Multi-Headed Attention
19
MULTI-HEADED ATTENTION

[Descripción de la imagen: atención multi-dirigida 1 atención multi-dirigida | Transcripción de la imagen: MULTI-HEADED ATTENTIÓON

Layer:( 55) Attention: (Input - Input — 3 Layer: | 5 41 Attention: | Input - Input — +

_- - "

The_

Multi-Headed Attention
19]]

POSITIONAL ENCODING
Representa el orden de los tokens en la secuencia tomada como input 
A cada embedding se le agrega un vector p
Los vectores p siguen un patrón que el modelo aprende para determinar la distancia entre tokens
Soy
un
estudiante
x0
x1
x2
p0
p1
p2
x0’
x1’
x2’
TOKENIZATION
EMBEDDING
POSITIONAL 
ENCODING
+
    +
        +
=
    =
        =
INPUT DEL ENCODER
20

[Descripción de la imagen: enoding posicional]

POSITIONAL ENCODING
p0
 PE(pos, 2i+1) = cos(pos/ 10000²i/d ) 
PE(pos, 2i)     = sen(pos/ 10000²i/d ) 
Donde:
pos es la posición del token en la secuencia
i es el índice del vector de posición
d es la dimensión de un embedding, la misma del vector de posición pi
d=4
i=0
   i=1      i=2      i=3
pos=0
21

[Descripción de la imagen: un diagrama que muestra la posición de una recal]

I am a student
OUTPUT
LINEAR
SOFTMAX
DECODERS
-2
...
1
...
7
-3
4
...
logits: puntaje de cada token
LINEAR
Red neuronal fully-connected que proyecta el 
output del último decoder en un vector más 
grande llamado logits
0.1
...
0.0
...
0.9
...
Probabilidad de cada token
22

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de los diferentes tipos de los diferentes tipos de los diferentes tipos de los diferentes]

I am a student
N ENCODERS
N DECODERS
OUTPUT
TRANSFORMER
Soy
un
estudiante
LINEAR
SOFTMAX
23

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de un mismo idioma]

03
ENTRENAMIENTO
¿Cómo aprende un 
Transformer?
24

[Descripción de la imagen: un fondo blanco y amarillo con las palabras entretenimiento]

CASO DE USO - TRADUCCIÓN
Se quiere traducir “gracias” a “thanks”.
Se tiene un vocabulario de salida v = (a, am, i, thanks, student, <eof>).
25

[Descripción de la imagen: case usaducon se en unavioo]

ENTRENAMIENTO
Los pesos se inicializan de forma aleatoria, entonces el modelo sin entrenar devuelve una 
distribución de probabilidades con valores arbitrarios para cada token 
Se pueden comparar las probabilidades y por backpropagation nos vamos acercando al 
output deseado. 
0.2
0.2
0.1
0.2
0.2
0.1
0.0
0.0
0.0
1.0
0.0
0.0
    a
am
i
thanks    student    <eof>
OUTPUT OBTENIDO
OUTPUT DESEADO
26

[Descripción de la imagen: un diagrama que muestra el número de los números en la tabla]

COSTO
Entrenar un Transformer requiere de gran costo computacional
27
Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[Descripción de la imagen: cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita cita | Transcripción de la imagen: COSITO

Entrenar un Transformer requiere de gran costo computacional

6 Results

6.1 Machine Translation

On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is
listed in the bottom line of Table 3. Trainingtook 3.5 dayson s PIOO:GPUs: Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.

Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.

27]]

VARIACIÓN DE PARÁMETROS
28
Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[Descripción de la imagen: varcion de parametos | Transcripción de la imagen: VARIACIÓN DE PARÁMETROS

Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to

per-word perplexities.

N — duoil de h de dy Pon “Els steps | (dev)

base | 6 512 2048 8 64 6 0.1 0.1 100K
I 512 52
4 128 128
(4) 16 32 32
32 l16 l16
(B)
(O) 32 32
128 128
0.0
0.2
D) 0.0
0.2
(E) positional embedding instead of sinusoids

big | 6 1024 40% 16 0.3 300K

Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.

train | PPL BLEU

(dev)
25.8
24.9
25:5
25.8
25.4
25.1
25.4
23.7
25.3
25.5
24.5
26.0
25.4
26.2
24.6
Za
25.3
25.7
25.7
26.4

params
109

65

213

28]]

APLICACIONES  
¿Para qué puedo usar un 
Transformer?
04
29

[Descripción de la imagen: un fondo blanco y amarillo con las palabras alciones paras]

4.1
TEXTO
Aplicaciones
30

[Descripción de la imagen: una captura de pantalla de un texto con el texto de texto en el medio]

BARD TEXT GENERATOR
31
TRANSFORMER
Gutenberg
Corpus
Red Neuronal
Nueva Historia

[Descripción de la imagen: un diagrama que muestra los diferentes tipos de datos]

CORPUS
32
POPULAR

[Descripción de la imagen: una serie de libros con diferentes títulos | Transcripción de la imagen: CORPUS

JANE AUSTEN

VAMPIRES

ColinWilson

Bram Stoker

THESPACE CLARIMONDE DRACUL Al

vampyre€

marchdale

»admiral

reelinsl Y aNC-1:S]]

GENERACIÓN
Época 1/100: “This story is about a woman”
Época 2/100: “This story is about a jungle for one , that he was the”
…
Época 20/100: “This story is about a good fortune and messages”
…
Época 60/100: “This story is about a to new truth , and be impossible for _us _ to visit”
…
Época 100/100: this story is about a man of the to names of ten years say ,     
33

[Descripción de la imagen: un fondo blanco con una fuente azul y negro que lee genacion]

GENERACIÓN
A small, tightly wound body found in San Bernardino County that police say may have 
come from the former San Bernardino shooter.
Police at San Bernardino County State Police began searching for the 32-month-old San 
Bernardino resident, who it said was in the early morning hours of Nov. 7, 2015.
Authorities were also interviewing a man the county's ﬁrst responders found at the scene.
The woman's death is the latest in a string of shootings in
34
ÉPOCAS
MODELO
10 
FineTuned
MYSTERY CORPUS

[Descripción de la imagen: un fondo blanco con un texto azul y blanco que lee,'un hombre soltero en un traje está de pie]

GENERACIÓN
A dark shadow has been found in the sky. The ghostly creature's name has not been found 
in the sky with no explanation, and no one knows the cause of it.
You can see it coming out of your mouth on this little piece of paper. It's tiny, yet you need 
only look at it to know if it actually was the ghost of the man who had killed the Earth
She was in the throes of madness and insanity. He had her hands clenched around her 
neck and she could hear her breathing on the ﬂoor of his room.
"Ahm…what's your name?" I looked over at her face. "Ahm…you haven't seen me like before." 
35
ÉPOCAS
MODELO
10 
FineTuned
VAMPYRE CORPUS

[Descripción de la imagen: un poema con una foto de una mujer en el fondo]

TEXT COMPLETION
He loved her so much that she cried and tried again to express her grief and to let that 
tears fall upon her heart; not only because of the great loss on which she took at once to 
love herself, but because of the great sorrow that had had arisen upon her by accident; and 
because of the great grief she had caused at once to love herself, as an expression of 
remorse when she was taken as such; when she cried and cried again, because of the 
great joy that had arisen upon
36
ÉPOCAS
MODELO
10 
FineTuned
LOVE CORPUS

[Descripción de la imagen: un poema con corazón y las palabras que lo dicen]

37
CHAT-GPT
Generative Pre-trained Transformer

[Descripción de la imagen: un texto verde y blanco con las palabras ``en rosa en negrita | Transcripción de la imagen: 1u
0)
E
—
1e
U
c
1qo)
—
T
Lo

Ine

Generative Pre-tra]]

38
OpenAI - API Docs
OpenAI - Examples
OpenAI - Playground
OPEN AI

[Descripción de la imagen: abierto - fondo - abierto - fondo - abierto - fondo - abierto - fondo - abierto - fondo - abierto - abierto - fondo abierto - abierto | Transcripción de la imagen: OPEN Al

OpenAl - Playground

OpenAl - API Docs
OpenAl - EXxamples

Classify the sentiment in these tweets:

1. "I can't stand homework"
2. "This sucks. I'm bored "
3."Ican't wait for Halloween!!!"

4."My catis adorable Y Y"
5. "Ihate chocolate"

Tweet sentiment ratings:

1. Negative
2. Negative
3. Positive
4. Positive

5. Negative

The following is a conversation with a teacher. The teacher is helpful and clever
Teacher: Hi there! How can I help you?
Student: Tm having trouble with my math homework.

Teacher: Let's take a look. What are you stuck on?

Translate this into French, Spanish and Japanese:
What rooms do you have available?

French: Quels sont les chambres que vous avez disponibles?
Spanish: ¿Qué habitaciones tienen disponibles?

Japanese: 5414 C ABEZ HELTUE A?|
38]]

39
GPT4
GPT 4 Developers Demo

[Descripción de la imagen: una imagen en blanco y negro con las palabras gpt 4 abierto como sistema avanzado, poc | Transcripción de la imagen: GPT4

GP T-4 is OpenAl's most advanced
system, producing safer and more useful

responses

39]]

4.2
IMÁGENES
Aplicaciones
40

[Descripción de la imagen: la imagen se muestra en el editor de imágenes]

PAPERS
41

[Descripción de la imagen: una página con el texto y la imagen de una persona | Transcripción de la imagen: PAPERS

A Transformer-Based Anomaly Detection System
| for OCT Image Embeddings

1 Eugenia Sol Piñeiro
Dpto. Ingeniería Informática

2 Ariel Schlaen
Sección Uveítis, Servicio de Oftalmología

3 Rodrigo Ramele
Dpto. Ingeniería Informática

Instituto Tecnológico de Buenos Aires Hospital de Clinicas “José de San Martín” Instituto Tecnológico de Buenos Aires

Buenos Aires, Argentina
epinciro Gitba.edu.ar

Variational Autoencoder as a Data Augmentation
tool for Confocal Microscopy Images

1* Eugenia Sol Piñeiro
Dpto. Ingeniería Informática

Buenos Aires, Argentina
epineiroCitba.edu.ar

Abstract—Retinoblastoma is an ocular tumor characterized by
malignant cells in the retina of the eye. For its treatment, doctors
apply different methods, one of them is the localized injection of
a chemotherapy drug called Topotecan. Scientists in the area
of medicine are interested in study the speed of Topotecan
penetration in tumor clusters. In order to do this, several
sequences of microscopy images of Retinoblastoma cell cultures
were taken. However, this process is very complex because
Retinoblastoma's tumor clusters require stringent growth and
maintenance conditions. That is why there is only one reduced set

of images, unique in Argentina. Nonetheless, for this same reason
IN Ds d e aa MA i oel

2"4 Rodrigo Ramele
Dpto. Ingeniería Informática
Instituto Tecnológico de Buenos Aires  Instituto Tecnológico de Buenos Aires
Buenos Aires, Argentina
rramele Citba.edu.ar

3 Juliana Gambini
Centro de Investigación CIDIA - CEPSI
Universidad Nacional de Hurlingham
Universidad Tecnológica Nacional
Universidad Nacional de Tres de Febrero
Buenos Aires, Argentina
juliana.gambini € unahur.edu.ar

out a predictive analysis and evaluate the results obtained for
drawing conclusions.

Hence, in this work we are proposing the usage of Varia-
tional Autoencoder to augment this dataset, aiming to imitate
the process of Topotecan penetration, while using Transform-
ers to verify that this imitation preserves the dynamics found
in the original dataset.

IL. BACKGROUND

y QA E AEO NAA EE + A ME F PA 2N S MCR - PA EA

Universidad de Buenos Aires
Buenos Aires, Argentina

Buenos Atres, Argentina
rramele Gitba.edu.ar

ariel.schlaen Egmail.com

4 Juliana Gambini
LIDEC, UNaHur

lad Nacional de Hurlingham

vidad Tecnológica Nacional

e Buenos Aires, Argentina

la-gambini E unahur.edu.ar

¡cal alter-
ete vision
diabetes,
oimmune
tect and
oimmune
ases that

1 for de-
; using a
18 Trans-
Il images,
to assist
ing early
es.

p Learn-

P—E 2..AAI2I)2)..2¿

visual transduction by converting light into electrical signals.
These signals travel through the optic nerve to the brain, where
they are interpreted to form visual images. The disruption of
the outer retina can result in vision loss, which may become
a complete loss if not identified and treated at an early stage.

This study arises from the need to estimate the likelihood
of a patient having retinal damage caused by AIR by detecting
the differences from other similar diseases. The first step to do
SO, is to propose a method to identify pathological from non-
pathological retinal images, using a Transformer-based Deep
Leaming model capable of capturing subtle clinical variations.

The Transformer has been widely used since its introduction
in [2] for several applications, such as Natural Language
models or Computer Vision, among others. In this work, it
is used to develop an image feature detection of AIR disease.

41]]

RETINOBLASTOMA
42
Tumor intraocular caracterizado por células malignas en la retina
Piñeiro, E. S. et al
RETINOPATÍA AUTOINMUNE
Alteraciones patológicas en la retina que pueden llevar a una 
pérdida parcial o total de la visión

[Descripción de la imagen: retoia alumina]

43
DALL-E 3
Enero
2021
Enero 
2022
DALL-E
CLIP
Abril
2022
DALL-E 2
Agosto
2023
DALL-E 3
Octubre
2023
DALL-E 3
+ ChatGPT 
y BingChat
Ejemplos
DALL-E 3

[Descripción de la imagen: un calendario con la fecha del día | Transcripción de la imagen: DALL-E 3

Enero Enero Abril Agosto Octubre
2021 2022 2022 2023 2023
DALL-E CLIP DALL-E ?2 DALL-E 3 DALL-E 3

+ ChatGPT

Research y BlngChat

DALL:E 3 understands significantly more nuance and detail

than our previous systems, allowing you to easily translate _ _

your ideas into exceptionally accurate images. " DALL-E 3

V2 $.
¡ Read research paper 7 | Try in ChatGPT 7 7' :-( -_ 7 n E | em º | OS
rrereéeéáJ»]TÁTÉTÉáfftftteeTtTReáeTKiIT[TZZRIZZZZ I É 7 e 7]]

44
IMAGE GENERATION

[Descripción de la imagen: la página de creación de imágenes en la aplicación mac | Transcripción de la imagen: IMAGE GENERATIÓN

Adobe Firefly

Lo mejor para dejar volar tu
imaginación

Utiliza la inteligencia artificial generativa y las
instrucciones sencillas de texto para crear imágenes
bonitas, efectos de texto y paletas de colores vivos con
la máxima calidad. Crea contenido innovador a partir de
a lann An enbanvan ia y1 loo aal a =

<ANVIDIA. > Main Menu

Generative Al for Visual Applications

NVIDIA Picasso

A foundry for building and deploying generative Al image,
video, 3D assets and 360 HDRi

Generate

44]]

45
GPT4-o
OpenAI - GPT4-omni

[Descripción de la imagen: la pantalla con el botón hello capt resaltado | Transcripción de la imagen: GPT4-0

OpenAl - GPT4-omni

Research Products Safety Company

May 13, 2024

Hello GP T-40

We're announcing GPT-40, our new flagship model that
can reason across audio, vision, and text in real time.

Contributions > TryonChatGPT 2 ITryinPlayground? Rewatch live demos >]]

4.3
AUDIO
Aplicaciones
46

[Descripción de la imagen: 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 | Transcripción de la imagen: 4j

AUDIO

Aplicaciones]]

WHISPER
47
Open AI - Whisper model

[Descripción de la imagen: una página con el texto «whser» | Transcripción de la imagen: WHAHISPER

Open Al - Whisper model

Robust Speech Recognition via Large-Scale Weak Supervision

Alec Radford ' Jong Wook Kim”' Tao Xu' Greg Brockman' Christine McLeavey' Ilya Sutskever '

Abstract

We study the capabilities of speech processing
systems trained simply to predict large amounts of
transcripts of audio on the internet. When scaled
to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zero-
shot transfer setting without the need for any fine-
tuning. When compared to humans, the models
approach their accuracy and robustness. We are
releasing models and inference code to serve as
a foundation for further work on robust speech
processing.

1. Introduction

Progress in speech recognition has been energized by the
development of unsupervised pre-training techniques exem-
plified by Wav2Vec 2.0 (Baevski et al., 2020). Since these

methods are exceedingly adept at finding patterns within a
training dataset which boost performance on held-out data
from the same dataset. However, some of these patterns are
brittle and spurious and don't generalize to other datasets
and distributions. In a particularly disturbing example, Rad-
ford et al. (2021) documented a 9.2% increase in object
classification accuracy when fine-tuning a computer vision
model on the ImageNet dataset (Russakovsky et al., 2015)
without observing any improvement in average accuracy
when classifying the same objects on seven other natural
image datasets. A model that achieves “superhuman” per-
formance when trained on a dataset can still make many
basic errors when evaluated on another, possibly precisely
because it is exploiting those dataset-specific quirks that
humans are oblivious to (Geirhos et al., 2020).

This suggests that while unsupervised pre-training has im-
proved the quality of audio encoders dramatically, the lack
of an equivalently high-quality pre-trained decoder, com-
bined with a recommended protocol of dataset-specific fine-
tuning, is a crucial weakness which limits their usefulness
and robustness. The goal of a speech recognition system

47]]

MÚSICA
48
Music Transformer: Generating Music with Long-Term Structure

[Descripción de la imagen: transformador de música, creando con palabra - enquie | Transcripción de la imagen: MUÚSICA

Music Transformer: Generating Music with Long-Term Structure

BIBIC

Home MNews Sport Business Innovation Culture Travel Earth Video Live

Rapper Bad Bunny has released a furious rant about a viral TikTok song that uses
artificial intelligence (AI) to replicate his voice.

The track has hundreds of thousands of views and also uses fake vocals from Justin
Bieber and Daddy Yankee.

In a post on his WhatsApp channel, Bad Bunny said anyone who liked the song
NostalgIA should leave the group chat.

"You don't deserve to be my friends," the singer wrote in Spanish. "I don't want them
on the tour either."

The track was uploaded by a user under the name flowgptmusic, but there's no
evidence to say they're linked to the AI platform FlowGPT - which is poweredina
similar way to ChatGPT.

48]]

4.4
APLICACIONES
Otros
49

[Descripción de la imagen: un fondo blanco y amarillo con las palabras 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]

APLICACIONES 
●
Comprensión, generación y traducción de textos: OpenAI Language Model 
●
Predecir la siguiente palabra de una oración:  IntelliSense in Visual Studio Code 
●
Juegos de Estrategia Real-Time:  AlphaStar - StarCraft II
●
Detección de Anomalías: Spacecraft Anomaly Detection
●
Reconocimiento de Imágenes:  Patches Are All You Need? 
     An Image is Worth 16x16 Words
●
Generar música: Music Transformer: Generating Music with Long-Term Structure
50

[Descripción de la imagen: aplicas conspe cono]

05.
INDUSTRIA
¿Cómo se usa hoy en día?
51

[Descripción de la imagen: la palabra en el editor de wordpress]

DEEP LEARNING FRAMEWORKS
52

[Descripción de la imagen: un fondo blanco con las palabras marcos de aprendizaje profundo y kras | Transcripción de la imagen: DEEP LEARNING FHRAMEWORKS

Keras

1F TensorFlow 6 PyTorch]]

HUGGING-FACE
53
Existen distintos modelos de Transformers: GPT-2/3/4, BERT, RoBERTa, DistilBERT, T5

[Descripción de la imagen: la interfaz del plug de wordpress | Transcripción de la imagen: HUCGCGING-FACE

Existen distintos modelos de Transformers: GPT-2/3/4, BERT, ROBERTA, DistilBERT, T5

* Transformers -

v4.3.0 v ENV € C) 114,893

GET STARTED

Transformers

Quick tour

Installation

TUTORIALS

Run inference with pipelines
Write portable code with AutoClass
Preprocess data

Fine-tune a pretrained model

E Transformers

State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.

 Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using
pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to

train a model from scratch. These models support common tasks in different modalities, such as:

» Natural Language Processing: text classification, named entity recognition, question answering, language
modeling, summarization, translation, multiple choice, and text generation.
Ed Computer Vision: image classification, object detection, and segmentation.
$: Audio: automatic speech recognition and audio classification.
£ Multimodal: table question answering, optical character recognition, information extraction from scanned

documents, video classification, and visual question answering.

53]]

MLOps CLOUD
54

[Descripción de la imagen: cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud cloud clo | Transcripción de la imagen: MLOps CLOUD

aWws

Machine learning Amazon SageMaker

Image recognition Amazon Rekognition
— . - . Speach Amazon Polly, Amazon
— Azure Q Sign in E

Natural language Amazon Comprehend

processing

Big Data Analytics Databricks

Chat Bot AWS Chatbot

Hugging Face on Azure

aws
—

Transformers models in Azure Machine Learning using Al applications
Hugging Face endpoints.

Try Azure for free

lowering costs.

Products Solutions Pricing Documentation Learn Partner Network AWS Marketplace

Azure Machine Learning
Azure Cognitive Services

Azure Cognitive Services

Azure Cognitive Services

Databricks

Azure Bot Service

Innovate faster with new capabilities, a choice of industry-leading FMs, and
infrastructure that pushes the envelope to deliver the highest performance while

Already using Azure? Try Hugging Face on Azure > = e
Amazon Q Amazon Bedrock
Customize this generative Al-powered assistant for Easily build and scale applications with LLMs, FMs,
the needs of your business and generative Al tools

»,
Google Cloud Al Platform
Google Cloud Vision
Google Cloud
Speech-to-Text and
Text-to-Speech

Google Cloud Natural
Language API:

Databricks

Dialogflow

Customer Enablement Events Explore More Qa

Deploy tens of thousands of pretrained Hugging Face Tools to build and scale generative

Explore more generative Al tools

Service

Amazon SageMaker

Bulld, train, and deploy your own FM models at
scale

54]]

BIBLIOGRAFÍA  
¿Dónde puedo profundizar?
06
55

[Descripción de la imagen: un fondo blanco y amarillo con las palabras bioraa]

Attention Is All You Need - Paper
GPT-2 - GPT-3 - BERT
Inside Machine Learning
Illustrated Transformer
Positional Encoding - Visualization
Word Embedding - Visualization
Simple Transformer in Python - Github
Interactive Transformer
BIBLIOGRAFÍA
56

[Descripción de la imagen: biocaa - bi - bi - bi - bi - bi - bi - bi - bi bi bi bi bi bi bi bi]

CÓMO PROFUNDIZAR
57
73.64 Temas Avanzados en Deep Learning
●
LLMs
●
Responsible AI y Safety 
●
Industria

[Descripción de la imagen: com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com com]

¡ GRACIAS ! 
58

[Descripción de la imagen: una tarjeta con la palabra gras en ella | Transcripción de la imagen: ¡ GRACIAS |]]

