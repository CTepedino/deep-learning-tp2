Deep Learning
Rodrigo Ramele
Lab NeurotrÃ³nica
Departamento de IngenierÃ­a InformÃ¡tica
Biomimetic Neuroscience

[DescripciÃ³n de la imagen: el logo de la nueva marca, la nueva marca]

Control
Mechanics
Informatics
Electronics
Neuroscience

[DescripciÃ³n de la imagen: los cuatro elementos del centro cientÃ­fico | TranscripciÃ³n de la imagen: Neuroscience]]

The Brain

[DescripciÃ³n de la imagen: la lluvia por la lluvia | TranscripciÃ³n de la imagen: 1he bPrain]]

[DescripciÃ³n de la imagen: una imagen computarizada de un cerebro]

Herculano-Souzel (2009)
FrenologÃ­a

[DescripciÃ³n de la imagen: el cerebro y sus funciones | TranscripciÃ³n de la imagen: , star-nosed
smoky ShÂº: s hamster mole eastern mole
shrew swew

a ea Â»Â» â€œÂ» a â€ ea
0.1769 0.3479g 04169 1.00g 0.802g 18029 0.999g
36 M 52 M 71 M 90 M 131 M 200 M 204 M

guinea pig marmoset agouti owl monkey

3.759g 7.789

240 M 534 M 18.365 g 15.73g

857 M 1468 M

capybara squirrel monkey
capuchin monkey

Thr reasor: this man la an vrreliat m tand ia Se
cajn de de very weni ln Confogality ad Parewrn) Love , y
1aj excecdiaky strong n Aritivoenmess Yousg ladics,
bewarr of suck men as ia satd:

Fren010gÃ­a macaque monkey

Herculano-Souzel (2009)]]

[DescripciÃ³n de la imagen: un cerebro con un fondo negro | TranscripciÃ³n de la imagen: Mark and Mary Stevens Keck Schoc

PIay TOLMaging and Informatics Institute Medicine 0f]

Thanks Phineas![60]

[DescripciÃ³n de la imagen: un crÃ¡neo y un hombre con un cuchillo | TranscripciÃ³n de la imagen: 1hanks Phineas!..

.1)

J'u

Z .-."F'l
Â«]]

Divisions and Broadmann Mappings

[DescripciÃ³n de la imagen: un cerebro con los nombres de diferentes partes]

Primary Areas

[DescripciÃ³n de la imagen: el cerebro es una parte del cerebro humano | TranscripciÃ³n de la imagen: Primary Arcas

Primary motor cortex

Supplementary motor area Central sulcus

Premotor area Somatosensory cortex

Posterior parietal cortex

Visual cortex

Lateral fissure]]

Primary Areas

[DescripciÃ³n de la imagen: el cerebro es una estructura grande, colorida y compleja que es importante para el cuerpo humano | TranscripciÃ³n de la imagen: Primary Areas

Primary motor cortex Primary sensory cortex
(precentral gyrus) (postcentral gyrus)

Somatic motor association area
(premotor cortex)

Somatic sensory association area

y â€”
_Ã±f-'

N
d Cn
" __ .

Visual association
area

Prefrontal cortex â€” Â£

Broca's area
(production of speech)

Visual cortex

Auditory association area
_ Wernicke's area
Auditory cortex (understand speech)]]

Brain Ventricles and Cerebrospinal Fluid

[DescripciÃ³n de la imagen: el cerebro es una gran parte del ser humano | TranscripciÃ³n de la imagen: brain Ventricles and Cerebrospinal Fuid

Right lateral
Ventricie

Central part of _
left lateral ventricie Left lateral ventricie

Fourth ventricie]]

Hemispheres and Corpus Callosum - Network

[DescripciÃ³n de la imagen: Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferio izquierdo Hemisferi | TranscripciÃ³n de la imagen: Hemispheres and Corpus Callosum - Network

Right Left Left and Right Brain Functions
z inear OPicemE
l | | Science and mathl2)rder

Logical Logic A
Analytic thougb
Corpus callosum - â€”
Verbal
Digital | Color

_, Ã‰ Art and music

reativity Random

Intuition
Free Emotion

olistic thought

Right Brain Functions

Brain]]

Subcortical - firmware

[DescripciÃ³n de la imagen: el cerebro es una gran parte del cuerpo humano]

Occipital - Human GPU

[DescripciÃ³n de la imagen: el cerebro y el cerebro estÃ¡n etiquetados en el hemisferio izquierdo]

Frontal Area - Brocca
Natural Language Processing

[DescripciÃ³n de la imagen: frontal - procesamiento de lenguaje natural broc]

Frontal - Language

[DescripciÃ³n de la imagen: frontal regiÃ³n del cerebro frontal regiÃ³n del cerebro frontal regiÃ³n del cerebro frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n frontal regiÃ³n | TranscripciÃ³n de la imagen: â€” Wemicke'sarea â€”
Broca's area (area 22)

for speech
(area 44,45)

Angular
gyrus

temporal agyrus
of Heschl
(area 41) Area 42

A-1 region

Frontal - Language]]

Frontal - Neocortex

[DescripciÃ³n de la imagen: la corteza frontal es la parte central del cerebro | TranscripciÃ³n de la imagen: Frontal - Neocortex

The prefrontal cortex as a controller

o Collection of interconnected
neocortical areas in the frontal
lobe

o Performs executive functions
that are important for the
conscious control of thought
and action in accordance with X Te 7
internal goals - e â€”â€”â€”â€”â€”â€”]]]_â€”â€”â€”
Dorsolateral PFC: abstract â€” = E
reasoning Ã¡ problem solving

o  Orbitofrontal and medial cortex:
affective Ã©: motivational functions

O

hap Npubs miaaas n gowpublicacns'arh3137215-230hm]]

Temporal - Pre, Motor and Somatosensorial

[DescripciÃ³n de la imagen: temporal - p e moracnonosal]

Temporal - Hippocampus and Neurogenesis (RAM)
Krzisch, Temprana, Mongiat 2014

[DescripciÃ³n de la imagen: temporal - hippaapus negenis r / m]

Temporal - Tempo parietal junction
Vestibular Apparatus
Proprioceptive
Ocular
Somtosensorial
Internal
Model

[DescripciÃ³n de la imagen: temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal temporal]

Neuron

[DescripciÃ³n de la imagen: una pared blanca con la palabra nen en ella]

Neuron

[DescripciÃ³n de la imagen: la estructura del ne ne nem | TranscripciÃ³n de la imagen: 1|

% Dendrites

r

- Synapticcheft ' â€” =

" RougheR. | Axonal terminal â€” /
= Miilboay â€” a
â€” Polyribosomes , â€” â€” Node of Ranvier
â€” Ribosomes.. â€” Â¿l Ã­

ol Â¡||:l|:lnamtu5â€”'.1

Myelin Sieath
[Sctraann cl

Axon hillack
ucheus

Nucheolus
Membrane
Mierotubiile

Nucleus â€”

Echwore cel

i Microfilament
7 >Mierotubule

Dendrites]]

Hodgkin-Huxley Model
I = Cm
dVm
dt + gK(Vm âˆ’VK) + gNa(Vm âˆ’VNa) + gl(Vm âˆ’Vl)
Hodgkin  1952, Abott â€œTheoretical Neuroscienceâ€
I = âˆ’(Cm
dV
dt ) + Ie
A

[DescripciÃ³n de la imagen: un circuito con una tensiÃ³n y una tensiÃ³n | TranscripciÃ³n de la imagen: Hodgkin-Huxley Model

AV de
E A
dt A
dV
1= Cm dÃ­ r gK(Vm m VK) + gNa(Vm - VNa) + gl(Vm A Vl)

Hodgkin 1952, Abott â€œTheoretical Neuroscienceâ€]]

Plasmatic Membrane - Na+/K+ Pumps
ïƒŠMantiene el volumen de la cÃ©lula.
ïƒŠPuede funcionar en reversa.
ïƒŠCanal de K+ selectivo de cationes mediante afinidad de 
terminaciones con grupos hidroxilos con el K+ deshidratado.
ïƒŠSensible al voltaje
ïƒŠDetermina la permeabilidad.

[DescripciÃ³n de la imagen: membrana pasmica - na - k - bombas | TranscripciÃ³n de la imagen: Plasmatic Membrane - Na+/ K+ Pumps

| - Â¿
+Mantiene el volumen de la cÃ©lula. Ea 3Na
+Puede funcionar en reversa. _

* + *

MCIKVMIOMDOOIINY

.-

Naâ€ wmw*Ãºbuwvlvw5p ;
.
l
Ã³ e y-*
M â€
l' ,"'
H ATP
+
: Interior 2K+ Pi
1
A +(Canal de K+ selectivo de catilones mediante afinidad de
- terminaciones con grupos hidroxilos con el K+ deshidratado.
Na* - -
. +Sensible al voltaje
Canales de Â«fugaÂ»
K--Na-

+Determina la permeabilidad.]]

Membrane Potential
ïƒŠEl Na+ por los Canales de fuga positiviza el interior al 
entrar.
ïƒŠEl K+ por los Canales de fuga negativiza el interior al 
salir.
ïƒŠLa bomba de Na+-K+ saca 3 contra 2 (todos 
positivos) con lo que negativiza el interior.
V = âˆ’61â‹…log CNai
+PNa+ +CKi
+PK + +CClo
âˆ’PCl âˆ’
CNao
+PNa+ +CKo
+PK + +CCli
âˆ’PCl âˆ’
â›
â
âœ
â
â 
âŸ
V Na+
V K+
EcuaciÃ³n Nerst para cada ion.
[mv]
+61mV âˆ’94mV = âˆ’86mV âˆ’4mv= âˆ’90mv
V Cl-

[DescripciÃ³n de la imagen: membre portal de la venta | TranscripciÃ³n de la imagen: Membrane Potential

EcuaciÃ³n Nerst para cada 1on.

+El Na*+ por los Canales de fuga positiviza el interior al
â€” entrar V=â€”61â€”log(

y

CNa' PNa" + CK* PK* + CCI;PCI-
CNa;, PNa" + CK: PK* + CCI- PCI-

+El K+ por los Canales de fuga negativiza el interior al

salir, NA

+61mV - 94mV = -86mV - 4mv=-9%0nwv

+La bomba de Na*-K* saca 3 contra 2 (todos
â€” positivos) con lo que negativiza el interior]]

Membrane Potential
V = âˆ’61â‹…log CNai
+PNa+ +CKi
+PK + +CClo
âˆ’PCl âˆ’
CNao
+PNa+ +CKo
+PK + +CCli
âˆ’PCl âˆ’
â›
â
âœ
â
â 
âŸ
EcuaciÃ³n Nerst para cada ion.
+61mV âˆ’94mV = âˆ’86mV âˆ’4mv= âˆ’90mv
Na+
62 log 145
5
= 90mV
62 log 145
15 = 61mV
K+
62 log
5
140 = âˆ’90mV
Clâˆ’âˆ’62 log 110
4
= âˆ’89mV
Ca2+
31 log 2.5
10âˆ’4 = 136mV
31 log
5
10âˆ’4 = 146mV
Equilibrio para cada ion
V Na+
V K+
[mv]
V Cl-

[DescripciÃ³n de la imagen: un diagrama de un plano con un plano en la parte superior y un plano en la parte inferior | TranscripciÃ³n de la imagen: Membrane Potential

Equilibrio para cada 1on

- 145
Naâ€ 62 log == = 90mV

145
62 log â€” = 61mV
15

S

K+ 62 logâ€” = - 90mV
140
- 110

Cr â€”02 logT â€” â€” 59mV
+ 2.5

Ca 31 log = 1360mV
MO

EE = 146mV

Og De m

EcuaciÃ³n Nerst para cada 1on.

CNa' PNa" + CK* PK* + CCI;PCI-
CNa;, PNa" + CK: PK* + CCI- PCI-

V=â€”61-log(

l

V Na*

+61mV - 94mV = -86mV - 4mv=-9%0nwv]]

Action Potential

[DescripciÃ³n de la imagen: una caricatura de un hombre sentado en una mesa con un gato | TranscripciÃ³n de la imagen: Action Potential

neurons do not fire,
tThey get firedl]]

Action Potential
Repolarizacion
Despolarizacion
Reposo
Canales 
de Fuga
Bomba 
de Na-K

[DescripciÃ³n de la imagen: un diagrama de la acciÃ³n de la acciÃ³n de la acciÃ³n de la acciÃ³n de la acciÃ³n de la acciÃ³n de | TranscripciÃ³n de la imagen: bomba â€” cCompuerta
de Na-K de activaciÃ³n Na*

Compuerta
de inactivaciÃ³n
Reposo
(-90 mVv)

950998

Canales
de Fuga

Interior

Action Potential

(-90 a +35 mV) (+35 a -90 mv,

ActivaciÃ³n lenta

Reposo
(+35 a -90 mV)

(-90 mVv)

â€” y +60
SobreestimulaciÃ³n â€œ %
- 20 g a
E O -10 5 E
z * â€”60 g
a2 â€” -80 -
2 = -100-â€” $
219 0s
g .g Pospotencial
g g 0.01 positivo
00
0,001 â€” Potencial de acciÃ³n
100 â€” Cociente
de las conductancias
- 10 ' â€” Na*
.. - â€” K+
gNE 1
S o
90
Ã­ Â£ 01
â€”
oE
O - 0,01
0,005
O 0,5 1 1,5

Milisegundos

TAO 1A urc 11 LITINLAN aa ial - ".ar]]

Saltatory Conduction
ïƒŠVainas de mielina (oligodendrocitos, 
cÃ©lulas de Schwann)
ïƒŠHasta 100 m/s (sin mielina 0.25 m/s)

[DescripciÃ³n de la imagen: salr - conduction salr - conduction | TranscripciÃ³n de la imagen: N | Ã"_-___"xxÃ' â€” /Ã'

Saltatory Conduction

+Vainas de mielina (oligodendrocitos,
cÃ©lulas de Schwann)

- > X +Hasta 100 m/s (sin mielina 0.25 m/s)

- PA "

1
Citonlasma

' de la celula de Vaina de mielina Axoplasma

â€”

N Nucieo de la celula
N de Schwann . â€” â€” â€” â€” Ã‘â€”â€” ] â€”Ã‘â€”]]

NÃ³dulo de Ranvier
ONP - m A

.- - â€”
â€” __

-

NÃ³dulo de Ranvier

.]]

Synaptic Weights
Synapse
ïƒŠCanales por ligando (40 neurotransmisores)
ïƒŠIÃ³nico: 
ïƒŠExcitatorio (Na+)
ïƒŠInhibitorio(Cl-, K+)
ïƒŠActivaciÃ³n de Segundo Mensajero:
ïƒŠProteinas G cAMP, cGMP 
ïƒŠCcambios enzimÃ¡ticos, transcripciÃ³n, activaciÃ³n 
de canales K+
ïƒŠOne-Way

[DescripciÃ³n de la imagen: synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis synsis sy | TranscripciÃ³n de la imagen: Synaptic Werghts

Synapse

+Canales por ligando (40 neurotransmisores)
+lÃ³nico:

+Excitatorio (Naâ€+)

+Inhibrtorio (Cl, K+)
+ActivaciÃ³n de Segundo Mensajero:

E rOre nastE CNR TE N E

+Ccambios enzimÃ¡ticos, transcripciÃ³n, activaciÃ³n

de canales K+

+One-Way

Vesiculas transmisoras

;1â€Â¡ mfÂº'$/Â¡) !r

ProteÃ­nas
receptoras

Soma de la neurona

Hendidura
sinÃ¡ptica
(200-300

angstroms)]]

Synaptic Weights
Synapse
https://twitter.com/i/status/1502259198603808774

[DescripciÃ³n de la imagen: una imagen de una sustancia dorada y verde | TranscripciÃ³n de la imagen: Synaptic Werghts

https:/ / twitter.com/i/status/ 1502259198603808774]]

Synaptic Weights
Synapse

[DescripciÃ³n de la imagen: una cÃ©lula se une a una cÃ©lula | TranscripciÃ³n de la imagen: Synaptic Werghts]]

Sumation and Propagation
Sumatoria 
Temporal
Sumatoria 
Espacial
Corriente elÃ©ctrica 
ionica
Segmento inicial del axon 
(mayor presencia de 
canales)
(+) Gap Junction
(+) Ephatic Coupling
(+) Mechanical

[DescripciÃ³n de la imagen: un diagrama de la estructura del ser humano]

Neural Networks

[DescripciÃ³n de la imagen: redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes neurales redes | TranscripciÃ³n de la imagen: y and
-
â€

U
=
.
>
_a
e
V
v
o Â»
>
O]]

Sumation and Propagation
Hyperpolarized pulse of current
Refractory Period
Rebound Spike
Temporal Summation

[DescripciÃ³n de la imagen: una lÃ­nea de radiaciÃ³n se muestra en el diagrama]

Cortical Networks
Barral J & Reyes AD (2016)

[DescripciÃ³n de la imagen: un diagrama de la red neuronal]

Brain Imaging

[DescripciÃ³n de la imagen: una pared blanca con la palabra 'bri lingg ' | TranscripciÃ³n de la imagen: brain Imagmng]]

Medical Imaging

[DescripciÃ³n de la imagen: un diagrama del cerebro y sus funciones | TranscripciÃ³n de la imagen: aging

Medical lm

EEG electrode
X8kull

TOPMNEC in Dintartralara.s]]

Invasive Scanning
Utah  Grid Microarray
Deep Brain Stimulation
PMT Corporation, Chanhassen, MN, USA
Elon Musksâ€™ Neuralink

[DescripciÃ³n de la imagen: una computadora y algunos componentes electrÃ³nicos | TranscripciÃ³n de la imagen: Figure 3: The robotic eectrede inserter; entarged view ei the inserter-head shewn in the inset A, Loaded neecie
pincheccartridye, B. Low-force contact brata positon sensor. C. Light modules with multipls independen: wavelengths,
D. Needle motor E. One of four cameras focuseÃ³ 0n the needle during insertion. F. Camera witk wide angle view of
vergical felÃ³. G Serenscapic camenas

500U?"|

Elon Musksâ€ Neuralink

Deep Prain Stimulation

PMT Corporation, Chanhassen, MN, USA]]

Non-invasive Scanning

[DescripciÃ³n de la imagen: non - sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans sans | TranscripciÃ³n de la imagen: Non-invasive Scanning

. EEG ElectroencefalogrÃ¡ma

. ECoG Electrocorticography

. MEG MagnetoencefalografÃ­a

. FMRI Resonancia MagnÃ©tica Funcional

. PET TomografÃ­a por EmisiÃ³n de Positrones

. FTCD Functional TransCranial Doppler

. INIR functional Near InfraRed]]

Non-invasive Scanning - EEG

[DescripciÃ³n de la imagen: un grupo de personas con auriculares y auriculares]

Non-invasive Scanning - fNIR MEG
University of Nottingham, 2018
Drexel University, 2016

[DescripciÃ³n de la imagen: dos personas usando mÃ¡scaras y una estÃ¡ sosteniendo un cuchillo]

Diffusion Tensor 
Trachtography
The tools for the Connectome

[DescripciÃ³n de la imagen: TensorografÃ­a de difusiÃ³n de la red de la conciencia]

Brain GPS
Moser, 2008

[DescripciÃ³n de la imagen: Mapas cerebrales y mapas cerebrales]

The materialization of Statistical Mathematics
Machine Learning
â€œLearningâ€ computers

[DescripciÃ³n de la imagen: lo natural de las marcas sÃ³lidas]

Machine Learning Model

[TranscripciÃ³n de la imagen: Machine Learning Model

_ Feature _ _
Preprocessing Classification

| Extracton
Signal Â¡

Acquisition

- Application
| System
â€ Closed
Loop |
Feedback

Volitional
Control]

Engineering Inverse Problem
Model
yK[n]
xN(t)
Ì‚xN(t) = f(sL(t))
xN(t)
+ Î¾N
ADC
Ì‚sM[n]
sL(t)
xN[n]

[DescripciÃ³n de la imagen: soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas soluciÃ³n de problemas | TranscripciÃ³n de la imagen: Engineering Inverse Problem

x [n]

N
0 â€” O â€” â€”

1 N( = fist(0) + EN Â¿ u
x (1)

s (1) $In]]]

What is â€œlearningâ€?
Model Free Approach
To determine the mapping 
between some input X and 
some output Y by 
â€œadjustingâ€ free parameters.
These free parameters are 
adjusted based on an 
Optimization Process

[DescripciÃ³n de la imagen: Â¿QuÃ© es aprender??????????????????????????????????]

What is â€œlearningâ€?

[DescripciÃ³n de la imagen: Â¿QuÃ© es aprender?????????????????????????????????? | TranscripciÃ³n de la imagen: What1s â€œlearning *

OPTIMIZATION PROBLEMS WITH CONSTRAINTS

(1) A company wants to manufacture cylindrical aluminum cans with a volume

of 1000em* (1 liter). What should the radius and height of the can be to
minimize the amount of aluminum used?

Volume of a Cylinder: V = r*h
Surface Area of a Cylinder: S = 27r? + 27rh

We wish to optimize (minimize) the surface area S = 27r* + 27rh with
respect to the constraint 7r*h = 1000. From this latter condition we have

h = 1909 which gives

wr2 ,
S = 2mr* + 2nr (Âº) = 27r* + M
A
Differentiating,
2
S = 4nr â€” â€”0(2]0.
r

Observe that r = Vis a critical point. We solve the equation S' = 0, giving

S =0
4nr â€” â€”2030 =
r
2
47 = 0(2)0
"
47* = 2000
3 500
r=

n

5003
r=(_) |
TT

The derivative S is negative for r in the interval (0, (229) ' and positive for
r > (500)'/* so we see that r = (200)'

the equation h = Â£Âºâ€”f gives h =

is a minimum. For this value of r

1000
1r(5$3_9)'/3']]

Terminology
ML
DATA
DS
BI
Analytics
ML
DL
Visualizations
UX
Business
AI
Analysis
MLOps

[DescripciÃ³n de la imagen: un diagrama de una computadora con una gran cantidad de datos | TranscripciÃ³n de la imagen: Business

* * **

Analysis

Analytics

Visualizations

UX]]

DL Map

[DescripciÃ³n de la imagen: el mapa de la d m p p p | TranscripciÃ³n de la imagen: â€œSGÂ¡QI!C ;_ = '"Â¿ UexrlplqutIsÃ±ns DalaUÂ¡Ã±Ã±lÂ¡mÃ±m

y 'â‚¬B'Â¡osÂ¡amiiÂ¿Ã© Ã DrobnbilÃ­tyÂº$lalistÃ­cs , *'BusimsslniellÂ¡g_mce U
PN 7 N le .ptmlwns
â€” CÂºÂº=Âº'1Âº'Âºw'ÂºÂº = â€”â€”Â¿,,Â¿â€¦ wÂ¿Â¡":ÂºfÃ­ â€” N â€”":RÂ£';$'M:chfâ€”:"

â€” Bayesion â€” + ] .
l â€”.rln(;rmcc P â€”."_,. =tmuing *E yEA ME

s unnlitntiue
l Anulgsis

â€” â€”

UmÃ­zâ€œ8elÂ£eamÃ­Ã±g-

--â€”' _ â€” ? " 1â€”<
"_ i â€œE â€”
- r â€” _ . â€œ '<_
R de = _Da NC Se E|
xW

"BÂ¡gÂ¡)atÃ±cchmlogics

M$GC â€” J?ig E

0mkgfffmÂ£o'p$_ Docber â€” â€”

B' ineeri neering â€” -u fâ€¦Ã1!U;C= . _.
9 9 ])alalÃ­ascthÂ¿mgâ€¦[kâ€” TE aA

- "
â€” â€” - - _ ]

- _ ÃNMSÃâ€¦$z â€” -â€”fâ€”__s â€5GÂ£Â¿-. < 'Dulnnr(lie:!uer _ * A D=
_ &ompuÂ¡er : ? , |
7 â€¦â€¦h< Â« '(; -1_: â€” - ul â€”h-

â€” =a

Qompnkr $cÂ¡ence â€”â€ÂºÂº"'"*

_= v'*h
1I-l
u

'ÂºrogmmmÃ­ng]]

DL Map

[DescripciÃ³n de la imagen: un diagrama del proceso | TranscripciÃ³n de la imagen: â€”DBSCAN

Agglomerative
FP Growth
Trees

Logistic
Regression

Linear Polynomial

Regression Regression
â€”
â€œ eaming â€”
- . Learning |
Ridge/Lasso sj , !
Regression Regression | m Classification |

Dimensionality â€”

| Machine â€”. ' :
Learning m Â¡

Reinforcement Ensemble | .
Lea m|ng Leamu |ng Bagging

E â€”
Boosting
â€”

-AdaBoost
CatBoost

Convolutional Neural
Networks (CNN)

' Perceptrons l

m â€” Recurrent Neural | +Adv , | )
Networks (RNN) ersagÂ¡;lÂ£lelworksÃ¡ Autoencoders]]

Roles
Data Engineer
Data Scientist
Data Analyst

[DescripciÃ³n de la imagen: una persona de pie en el agua con las palabras 's O's ']

Evolving Artificial Intelligence
Statistical
Inference
Symbolic
AI
Theory of
Learning
Neural 
Networks
Computer
Vision
SVM
MLP
Machine
Learning
Deep
Learning
LDA
CNN
LSTM
GANs
Clusters
Control
Theory
Autoencoders
Datamining
Pearson
Fisher
Norvig
Hertz
Haykin
Goodfellow
Mitchel
Brunton
Szilesky
Vapnik
Zaki
Applied
Statistics
Transformers

[DescripciÃ³n de la imagen: un diagrama de un flujo de informaciÃ³n | TranscripciÃ³n de la imagen: Evolving Artificial Intelligence

h

Transformers

â€” E o â€” Ã“N =]]

Emerging Technologies
â–GANs
â–Explanaible AI
â–Edge AI
â–AI PaaS
â–Adaptive ML
â–Emotion AI

[DescripciÃ³n de la imagen: energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a | TranscripciÃ³n de la imagen: *

Ã“.Q

9
Ã“.Ã“

GANs
Explanaible Al
Edge Al

Al Paas
Adaptive ML

Emotion Al

Emerging l echnologies

Gartner Hype Cycle for
Emerging Technologies, 2019

Expectations

Biochips
AlPass *

Edge Analytics,
Autonomcus Oriving Leve!5 * . y
Low-Esrth-Orbit Satellite Systams..

Fdge Al.. â€” Graph Analytics
Explainable Al â€”
Personification â€” _
Knowiedge Graphs - -;_-7 â€” Next-Generation Memory

Synthetic Data â€”â€” 3D Sensing Cameras

Light Cargo Delivery Drones â€”

Iransfer Leaming â€” _

Flying Autonomous Vehicles â€” â€”
Augmented Intelligence

Nanoscale 3D Printing

â€” Emection Al

â€”

Autoncmous Driving Level 4

Decentrelized Autonomous â€” â€”

Organization â€”DigitalOps
Generalive Adversarial Adaptive ML
Networks â€” â€”
Decentrelized Web â€” â€”
AR Cloud-â€”

â€” Immeresive Workspacee

Biotech - Cultured â€”
or Artificial Tissue

Peak of
Innovation Inflated Trough of Slope of Plateau of
Trigger Expectations Disillusionment Enlightenment Productivity

Time

â€” less than ? years $ ? to5 years S 5to10 yaars C) more than 10 years â€” q bsalste befors plateaa As of August 2018

Pleteau will be resched:]]

Learning by Data
In ML there are three approaches to force a machine to â€œlearnâ€
Supervised Learning
Unsupervised Learning
Reinforcement Learning
Semi
Supervised 
Learning
Self
Supervised 
Learning
Classification
Regression
Generation
Optimize

[DescripciÃ³n de la imagen: aprendizaje de datos en el mismo enfoque para hacerlo mÃ¡s fÃ¡cil]

Machine Learning Techniques
SVM
LDA
LogReg
kNN
Autoregressive

[DescripciÃ³n de la imagen: un grÃ¡fico con una lÃ­nea que sube a la parte superior e inferior | TranscripciÃ³n de la imagen: Machine Learning l echniques

*x * a
x X * 8 8

8
ka 8

a

8
Â£
8

* Autoregressive -]]

Machine Learning Techniques
DBSCAN
k-means
OPTICS
SOM

[DescripciÃ³n de la imagen: un diagrama de una mÃ¡quina con un gato y un ratÃ³n | TranscripciÃ³n de la imagen: Machine Learning l echniques

.. *
...
* â€œ.
XXX *
.
a-
9 Â£]]

Watching Dreams?
Nishimoto,Cell 2011, Nytimes

[DescripciÃ³n de la imagen: un col de imÃ¡genes de personas y animales]

Watching Thoughts?
High-resolution image reconstruction with latent diffusion models from human brain activity, Takagi, Nishimoto 2022

[DescripciÃ³n de la imagen: un col de fotos de personas y animales]

Biomimetic neuroscience
Deep Brain
The Revolution of Deep Learning

[DescripciÃ³n de la imagen: las palabras lluvia profunda se muestran en rojo y blanco]

Neural Networks - McCulloch and Pitts

[DescripciÃ³n de la imagen: neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neur | TranscripciÃ³n de la imagen: Neural Networks - McCulloch and Pitts

Inputs  Weights

f i

l= e Output
: Threshold 7
A]]

Neural Networks - MLP Multilayer Perceptron
h = ğœ(ğ‘+ ğ‘¥ğ‘Š)
ğ‘œ= ğœ(ğ‘+ hğ‘‰)
ğ‘œ= ğœ(ğ‘+ ğœ(ğ‘+ ğ‘¥ğ‘Š)ğ‘‰)

[DescripciÃ³n de la imagen: redes neuronales - m - multipersona | TranscripciÃ³n de la imagen: Neural Networks - MLP Mululayer Perceptron

o=o(p+o(b+xMW)V)

oO = a(p+%V)

7

h=o(b+xW)]]

Deep Learning
(*) Turing Award 2019
Cybernetics (McCulloch and Pits, Hebb 1949, Rosenblatt 1958)
Connectionism (Backprop: Rumelhart and Hinton^, Hopfield^, 1986)
Dechter 1986
Aizenberg 2000
Krizhevsky AlexNet 2012
Deep Learning (Hinton, Bengio and LeCun* 2006, Ranzato 2007)
ChatGPT (OpenAI)
Minsky and Papert 1969
Olshausen and Field 2005
Fukushima Neocognitron 1980
Distributed Representation 1986
Beating ImageNet Krizhevsky 2012
Playing Atari Games, Mnih 2015
}
Origins
(^) Physics Nobel Prize 2024

[DescripciÃ³n de la imagen: Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido Ã¡cido]

What Deep Learning is NOT?
"Los algoritmos tienen ademÃ¡s la ventaja que no se 
cansan y pueden interactuar con el contexto a 
velocidades supersÃ³nicas en comparaciÃ³n 
con las personas"
â€œLos algoritmos de IA no se programan.â€
â€œNada serÃ¡ igualâ€ M.Tetaz
â€œDeep Learning permite reemplazar a los seres humanos.â€
â€œLos robots vienen a reemplazarnos.â€
â€œTrolley Train Problem.â€
Laura Accion, et al , â€œDesmistificando la Inteligencia Artificialâ€ en â€œInteligencia artificial, una mirada interdisciplinariaâ€ , 2020
Why the AI is so dumb, IEEE Spectrum Nov 2021
â€œAGI Artificial General Intelligence.â€
â€œLa tecnologÃ­a empezÃ³ a reemplazarnos, y mÃ¡s nos vale sacarle provecho que quedarnos amargados por la herida que nos provoque."
â€œMake decisions based on data.â€

[DescripciÃ³n de la imagen: Â¿QuÃ© no es??????????????????????????????????]

What Deep Learning is NOT?

[DescripciÃ³n de la imagen: Â¿QuÃ© sÃ©? Â¿QuÃ© sÃ©? Â¿QuÃ© sÃ©? Â¿QuÃ© sÃ©? Â¿QuÃ© sÃ©? | TranscripciÃ³n de la imagen: W hat Deep Learning 1s NO 1 *

MIT
T&Ch_â€œ0lÂºgy Featured Topics  Newsletters Â£Events 1 Podcasts Sign in Subscribe
Review

ARTIFICIAL INTELLIGENCE

Hundreds of Al tools have been built to catch

covid. None of them helped.

Some have been usedin hospitals, despite not being properly tested. But the
pandemic could help make medical Al better.]]

What Deep Learning is NOT?
https://pulsar.uba.ar/creencias-sociales-2024-informe-6-los-argentinos-y-la-ciencia/

[DescripciÃ³n de la imagen: lo que hacemos lo que hacemos lo que hacemos lo que hacemos lo que hacemos lo que hacemos lo que hacemos lo que hacemos | TranscripciÃ³n de la imagen: W hat Deep Learning 1s NO 1 *

* El desarrollo cientÃ­fico no es cosa de jÃ³venes: el 51% de las
personas menores de 30 aÃ±os no lo ubica entre las urgencias del
paÃ­s. Las generaciones mayores lo jerarquizan mÃ¡s.

La Inteligencia Artificial y el horizonte de lo humano: el 46% cree
que la lA puede superar a las personas, mientras que el 52% lo
descarta. Miedo, fascinaciÃ³n y futuro incierto.

https:/ / pulsar.uba.ar / creencias-sociales-2024-informe-6-los-argentinos-y-la-ciencia /]]

What Deep Learning is NOT?
https://pulsar.uba.ar/creencias-sociales-2024-informe-6-los-argentinos-y-la-ciencia/

[DescripciÃ³n de la imagen: Â¿QuÃ© no lo es? | TranscripciÃ³n de la imagen: W hat Deep Learning 1s NO 1 *

Esto muestra una sociedad escÃ©ptica ante la idea de que la lA supere al
ser humano, aunque no de manera tajante. Los mÃ¡s jÃ³venes (18-29
aÃ±os) son el Ãºnico grupo donde predomina la creencia en su
superioridad futura (55% estÃ¡ muy o algo de acuerdo). Esto puede
vincularse con una mayor familiaridad tecnolÃ³gica, exposiciÃ³n a
narrativas digitales y una socializaciÃ³n en un mundo donde la lA ya
opera cotidianamente. En cambio, a medida que se avanza en edad
crece el escepticismo. Entre los mayores de 50 aÃ±os de edad, la
mayorÃ­a rechaza esa posibilidad (55% estÃ¡ poco o nada de acuerdo),
quizÃ¡s como defensa simbÃ³lica del lugar humano o por menor
exposiciÃ³n y comprensiÃ³n al avance de estas tecnologÃ­as.

https:/ / pulsar.uba.ar / creencias-sociales-2024-informe-6-los-argentinos-y-la-ciencia /]]

What Deep Learning is?
Revolutionary Technology to move ahead the digital boundary
Computer 
Vision
Natural 
Language
Processing
Speech
Recognition
â€œArtificial Intelligence and Life in 2030â€, Sept 2016

[DescripciÃ³n de la imagen: Â¿QuÃ© es lo que es lo que es lo que es lo que es lo que es lo que es lo que es lo que]

Digital Boundary
Artificial Intelligence
Digital Automation
Automated
Not yet automated
Region
Average Human Capacity
Superhuman
Capacity
Digital Boundary

[DescripciÃ³n de la imagen: frontera digital frontera digital frontera digital frontera digital frontera digital frontera digital]

Only that ?
https://whatisintelligence.antikythera.org/
Computer 
Vision
Natural 
Language
Processing
Speech
Recognition
Blueprints
Thinking 
Machine

[DescripciÃ³n de la imagen: un diagrama de una computadora con las palabras en Ã©l | TranscripciÃ³n de la imagen: Only that *

Speech

Computer
/ulon Natural

Language

kecognition

f rocessing

https:/ / whatisintelligence.antikythera.org /]]

What Deep Learning is?
Nature,Wired, The Batch, Newscientist

[DescripciÃ³n de la imagen: Â¿QuÃ© es un sitio web?????????????????????????????? | TranscripciÃ³n de la imagen: W hat Deep Learning 1s?

Magnetic control of tokamak plasmas through deep
reinforcement learning

Jonas Degrave, Federico Felici , ... Martin Riedmiller + Show authors

How a Plucky Robot Found the Long-Lost Endurance

Shipwreck
Over a century ago, two dozen men were stranded in Antarctica. Here's how a robot dove 10,000 feet to glimpse their lost ship for the first
time.
AlphaFold AI predicts SARS-CoV-2 omicron . o
variant might not evade antibody Alph3F()ld. a solution to a
neutralization _ _
Discovering faster matrix multiplication algorithms 50 year Old grand
with reinforcement learning challenge in bi0]0gy
________________ â€”â€”â€” Alhussein Fawzi -7, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes,
N ] DeepMind's StarCraft-playing Al
J a 39 -â€”9-â€”06 beats 99.8 per cent of human

| gamers

...................

Competitive Coder Nature,Wired, The Batch, Newscientist]]

What Deep Learning is?

[DescripciÃ³n de la imagen: Â¿QuÃ© es lo que es lo que es lo que es lo que es lo que es lo que es lo que es lo que es lo que es lo que es]

A window to our imagination

[DescripciÃ³n de la imagen: un hombre en una camisa azul estÃ¡ de pie delante de un fondo blanco con las palabras gove2 | TranscripciÃ³n de la imagen: MGoogle Veo 2

CINEMATIC FOÂ©TAGE

N]]

Where Deep Learning may have something to do?
Tokyo University

[DescripciÃ³n de la imagen: Â¿Una cÃ¡mara de video con las palabras para las que tenemos algo? | TranscripciÃ³n de la imagen: W here Deep Learning may have something to do?

Â« Â¡

1 324 views A 0:24 /1:00 (]>) Â¡Â¿7'

Tokyo University]]

Agility Robotics
Where Deep Learning may have something to do?

[DescripciÃ³n de la imagen: Una mujer estÃ¡ usando una mÃ¡quina para hacer algo]

Boston Dynamics
Where Deep Learning may have something to do?

[DescripciÃ³n de la imagen: Â¿Un robot estÃ¡ de pie en una habitaciÃ³n con una seÃ±al que dice que estamos deseando tener algo que hacer? | TranscripciÃ³n de la imagen: boston Dynamics]]

Realistic Applications
â—
Aburrido
â—
Descuidado
â—
Peligroso
ãã¤ã„ãƒ»æ±šã„ãƒ»å±é™º
Rodney Brooks, â€œAn inconvenient truth About AIâ€, IEEE Spectrum, Sept 2021
Every successful deployment has either one of two expedients: 
â€¢ It has a person somewhere in the loop, or 
â€¢ The cost of failure, should the system blunder, is very low

[DescripciÃ³n de la imagen: aplicaciones realistas estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio estudio]

Deep Learning
Â IEEE Spectrum Nov 2021, Why AI is harder than we think?[49]

[DescripciÃ³n de la imagen: en los primeros dÃ­as de la revoluciÃ³n industrial | TranscripciÃ³n de la imagen: NEW NAVY DEVICR
LEARNS BY DOING

Psychologist Shows Embryo
of Computer Designed to
Read and Grow Wiser

WASHINGTON, July. 7 (UPI)
â€”The Navy revealed the em-
bryo of an electronic computer
today that it expects will be
able to walk, talk, see, write,
reproduce itself and be .con-
scious of its existence,

The embryoâ€”the Weather
Bureau's $2,000,000 *704â€ com-
puterâ€”learned to differentiate
between right and left after
fiftty attempts in the Navy's
demonstration for newsmen,,

The service said it would use
this principle to build the first
of its Perceptron thinking ma-
chines that will be able to read
and write, It is expected to be
finished in about a year at a
cost of $100,000.

Dr. Frank Rosenblatt, de-
signer of the Perceptron, con-
ducted the demonstration, He
said -the machine would be the
first device to think as the hu-
man brain. As do human be-

Deep Learning

ings, Perceptron will make mis-
takes at first, but will grow
wiser as it gains experience, he
said, '

Dr: Rosenblatt, a research
psychologist at the -Cornell
Aeronautical Laboratory, Buf-
falo, said Perceptrons might be
fired to the planets as mechani-
cal space explorers,

Without Human Controls |

- The Navy said the perceptron,

would be the-first non-living
mechanism â€œcapable of receiv-
ing, recognizing and identifying
its surroundings without -any
human training or control.â€ |

The â€œbrainâ€ is designed to
remember images and informa-.
tion it has perceived itself. Ordi-
nary computers remember only
what is fed into them on purch
cards or magnetic tape. . |

Later Perceptrons will be able
to recogniZe people and call out
their names and instantly trans-
late speech in one language to
speech or writing in another
language, it was predicted.

Mr, Rosenblatt said in prin-
ciple it would be possible to
build brains that could repro-
duce themselves on an assembly
line and which would be con-
scious of their existence. '

1958 New York
Times...

In today's demonstration, the
â€œ704â€ was fed two cards, one
with squares marked on the left
side and the other with squares
on the right side.

Learng by Doing

In the first fifty trials, the
machine made no distinction be-
tween them. It then started
registering a â€œQâ€ for the left
squares and â€œOâ€ for the right

squares. ,
Dr. Rosenblatt said he could
explain why the imachine

learned only in highly technical
terms. But he said the computer
had undergone a â€œself-induced
change in the wiring diagram.â€

The first fPerceptron will
have about 1,000 electronic
â€œassociation cellsâ€ receiving
electrical impulses from an eye-
like scanning device with 400
photo-cells. The human brain
has 10,000,000,000 responsive
cells, including 100,000,000 con-
nections with the eyes.

IEEE Spectrum Nov 2021, Why Alis harder than we think?[49]]]

Deep Learning
Â Why AI is harder than we think?[49]
1956 Dartmouth AI Conference

[DescripciÃ³n de la imagen: salir en casa con su familia]

The Four â€œDeep Learningâ€ pillars
Multiple Level of Composition
Representational Learning
Massive and standardized datasets 
GPU
Â The Hardware Lottery [44]

[DescripciÃ³n de la imagen: los cuatro pilares de aprendizaje de dep multinivel representaciÃ³n de la composiciÃ³n aprendizaje]

Why deep?
Limitations of Machine Learning on world problems
Â Hochreiter 1991, Hinton 2006
Â  Schmidhuber 2014

[DescripciÃ³n de la imagen: un fondo blanco con las palabras wh e p p p p p p p p p p p p]

ML Problem - The Curse of Dimensionality

[DescripciÃ³n de la imagen: ml polo - el uso de la densidad]

ML Problem - The Curse of Dimensionality

[DescripciÃ³n de la imagen: polo de ml â€“ el uso de la dimensionalidad]

ML Problem - The Curse of Dimensionality

[DescripciÃ³n de la imagen: ml polo â€“ el uso de la densidad | TranscripciÃ³n de la imagen: ML Problem - The Curse of Dimensionality

Dessert: High-Dimensional Intuition Failure

e In R*:
Â» 4 unit-radius circles inside [â€”2, 2]*

> circle of radius r centred at the origin,
confined by the unit-radius circles

Ã­

> distance from origin to centers: V2
Â» 7r=/2-1

o In R
> 24 unit-radius spheres inside [â€”-2, 2]â€œ

Â» radius-r sphere centred at the origin,
confined by the unit-radius spheres

e J. Michael Steele, The r= 1
Cauchy-Schwarz Master Class,
Cambridge, 2004. > for d > 10, we have r >2

If d > 10, the sphere confined by the spheres in the box, goes outside the box!

MÃ¡rio A. T. Figueiredo (IST Â£ IT) Statistical Learning: Lecture 1]]

Why deep?
â€œApproximation by superpositions of a sigmoidal functionâ€, Cybenko 1989
Universal 
Approximation 
Theorem
A feedforward neural network with a single 
hidden layer containing a finite number of 
neurons can approximate any continuous function 
on a compact subset of R, given appropriate 
activation functions.

[DescripciÃ³n de la imagen: Â¿Por quÃ©??? Â¿? Â¿? Â¿? Â¿? Â¿? Â¿? Â¿? Â¿?? Â¿??]

Why deep?
Shift Invariance
Locality at different scales
Limitations of ANN on real world problems
{
Compositional 
Structure
â€œMany natural questions on images corresponds to 
algorithms that are compositionalâ€
â€œThe unreasonable effectiveness of deep learning in artificial intelligenceâ€, Sejinowski 2019
Â â€œWhen and why..â€, Mhaskar 2017
Shallow Network
Deep Network
UAT
Difficult to train
Difficult to find the optimal
Difficult to generalize well

[DescripciÃ³n de la imagen: un diagrama de los dos tipos de la red]

Why deep?
The Curse of Dimensionality
Local Consistency Assumption
Interests points are Sparse
Fundamental Credit Assignment
Problem
Credit Assignment Path
Limitations of ANN on real world problems
Â Hochreiter 1991, Hinton 2006
}
Deep
Fundamental Deep Learning 
Problem
Â  Schmidhuber 2014
Vanishing or Exploding Gradients
Â  â€œWhen and why..â€, Mhaskar 2017
Limitations of 
Machine Learning 
on world problems

[DescripciÃ³n de la imagen: un diagrama de los diferentes tipos de la palabra]

Add more layers

[DescripciÃ³n de la imagen: Los admores Los admores Los admores Los admores Los admores Los admores Los admores Los admores Los admores | TranscripciÃ³n de la imagen: Add more layers

%

Gentlemen, our learner
overgeneralizes because the
C-Dimension of our Kernel
s too high, Get some
experts and minimze the
structural risk in a new one.
Rework our loss function,
ake the next kernel stable,
nbiased and consider using
t margin

â€œ1

NEURAL
NETWORKS

S TACK
MORE
LAYERS

.%Ã­Ã­Â»

LAYERS]]

Overcoming the FDLP
Limitations of ANN on real world problems
Â Hochreiter 1991, Hinton 2006
}
Fundamental Deep Learning 
Problem
â–Unsupervised Pretraining
â–Greedy Layer-Wise
â–LSTM cells
â–Hessian Free optimization, Dropout, Xavier initialization
â–Random weight guessing
Â  Schmidhuber 2014

[DescripciÃ³n de la imagen: un fondo blanco con un texto rojo y negro que lee,'''''''''''''''''''''''''''''''''''''''''''''''''']

Neural Networks - Convergence Paradox
â€œThe unreasonable effectiveness of deep learning in artificial intelligenceâ€, Sejinowski 2019

[DescripciÃ³n de la imagen: redes neuronales - parox de convergencia]

Representational Learning
Handcrafted Feature

[DescripciÃ³n de la imagen: un diagrama del cerebro humano y sus funciones | TranscripciÃ³n de la imagen: Representational Learning

Feature | : Â¿
Preprocessing E : PUIASSIICALION Y Handcrafted Feature
-XtrTacton

| Signal e Â¡
Acquisition

Application
System

Loop |

| Feedback

Volitional
Control]]

Representational Learning
In ML there are three approaches to force a machine to 
â€œlearnâ€
Supervised Learning
Unsupervised Learning
Reinforcement Learning
Semi
Supervised 
Learning
Self
Supervised 
Learning

[DescripciÃ³n de la imagen: un diagrama del proceso de aprendizaje de una persona | TranscripciÃ³n de la imagen: Representational Learning

In ML there are three approaches to force a machine to

â€œlearnâ€
Â© Supervised Learning Semi
t Supervised
Learning
Â© Unsupervised Learning
Selft

Supervised
Reintorcement Learning â€”

Learning

Output
(object identity)

3rd hidden laver

(object parts)

2nd hidden laver

(corners and

contours)

l st hidden layer

(edges)

Visible laver
( input pixels)]]

Massive Datasets
MNIST handwritten digit dataset
Iris 10^2 , ImageNet 5000 images per category, WMT 10^9 total size,
ImageNet Classification Error

[DescripciÃ³n de la imagen: fecha del instrumento msn | TranscripciÃ³n de la imagen: Massive Datasets

Qs Y MY S QNE

ImageNet Classification Error (Top 5)
Ka

2016

2015 (ResNet)

2014 (VGG)

)

13 (ZF

2011 (XRCE) 2012 (AlexNet)

(GoogLeNe

2014
(GoogleNet)

ImageNet Classification Error

MNIST handwritten digit dataset

Iris 1012 , ImageNet 5000 images per category, WMT 109 total size,]]

Mammalian Brain Like?
[18] MultiGPU ConvNet Krizhevsky 2012
[9] GoogLeNet Szegedy 2014

[DescripciÃ³n de la imagen: el diagrama muestra la relaciÃ³n entre los dos tipos principales del modelo | TranscripciÃ³n de la imagen: Connections per neuron

Mammalian Brain Like?

1985 2000 2015

[9] GoogLeNet Szegedy 2014

Number of neurons (logarit hnic scale)

1950 19855 2000 2015 2056

[18] MultiGPU ConvNet Krizhevsky 2012]]

GPU
CUDA
OpenCL
Shaders

[DescripciÃ³n de la imagen: cpu cpus y otros componentes | TranscripciÃ³n de la imagen: â€” Quaramer sivinos

CUDA aOpenCL

Shaders

Rotation

axis .
y Axis

Rotation
Plane

(toward viewer]]

GPUs
Intel Movidius Neural Stick
Google Coral Dev Board
Nvidia Titan V
Nvidia Jetson Tk1
Nvidia Jetson Nano. RPi para GPU, Hardware Lottery [44]

[DescripciÃ³n de la imagen: cpu cpus y otros dispositivos electrÃ³nicos]

Money is hardware

[DescripciÃ³n de la imagen: dinero es difÃ­cil en Internet | TranscripciÃ³n de la imagen: Money 1s hardware

ImageNet Training in 24 Minutes

Yang You, Zhao Zhang, James Demmel, Kurt Keutzer, Cho-Jui Hsieh
(Submitted on 14 Sep 2017)

Finishing 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU takes 14 days.
This training requires 10* 16 single precision operations in total. On the other hand, ihe world's
current fastest supercomputer can finish 2 * 1017 single precision operations per second
(Dongarra et al 2017). If we can make full use of the supercomputer for DNN training. we should
be able to finish the 90-epoch ResNet-50 training in five seconds, However, the current bottieneck
for fast DNN training is in the algorithm fevel. Specifically, the current batch size (e-g. 512) is too
smaill to make efficient use of many processors

For large-scale DNN training, we focus on using large-batch data-parallelism synchronous SGD
without losing accuracy in the fixed epochs. The LARS algorithm (You, Gitman, Ginsburg, 2017)
enables us to scale the batch size to extremely large case (e.g. 32K). Ve finish the 100-epoch
ImageNet training with AlexNet in 24 minutes, which is the world record. Same as Facebook's
result (Goyal et al 2017). we finish the 90-epoch ImageNet iraining with ResNet-50 in one hour.
However, our hardware budget is only 1.2 million USD, which is 3.4 times lower than Facebook's
41milion USD. â€” â€” â€”]]

Software and Cloud Based Tools
â€¢ Amazon: MachineÂ Learning, DSSTNE, Orbeus
â€¢ Facebook: TorchLibrary
â€¢ Google: Cloud MachineÂ Learning, TensorFlow
â€¢ IBM: Watson Analytics, IBM SystemsML (AlchemyAPI)
â€¢ Microsoft: Azure MachineÂ Learning, Computational Network Toolkit (SwiftKey)
â€¢ OpenAI
â€¢ Theano
â€¢ PyLearn2
â€¢ Torch
â€¢ DistBelief
â€¢ Caffe
â€¢ MXNet
â€¢ Tensorflow
â€¢ Keras
https://spectrum.ieee.org/computing/software/now-you-too-can-buy-cloudbased-deep-learning

[DescripciÃ³n de la imagen: software software software software software software software software software software software software software software software software software software software]

Keras and TensorFlow
C/C++ 
Numerical 
Routines
CUDA
OpenMP
Shaders
GPU
Python3
NumPy
TensorFlow
Theano
CNTK
Keras
Scikit
CPU
Google 
Colab
Python 
Scripts
Jupiter
Notebooks
R
Scipy
cython
OpenCL
OpenGL

[DescripciÃ³n de la imagen: un diagrama de una computadora con varios tipos diferentes | TranscripciÃ³n de la imagen: Keras and lensorFlow

=
â€”
EE
=
e
e
5
ha]]

Deep Learning Networks

[DescripciÃ³n de la imagen: redes de aprendizaje profundo redes de aprendizaje profundo redes de aprendizaje profundo redes de aprendizaje profundo redes de aprendizaje profundo redes de aprendizaje profundo redes de aprendizaje profundo]

ConvNet - Convolutional Networks
Representational Learning
Fukushima 1980, Krizhevsky 2012
â€¢ Pooling = downsampling
â€¢ ReLU: Rectified Linear Neurons

[DescripciÃ³n de la imagen: redes de control de connet | TranscripciÃ³n de la imagen: ConvNet - Convolutional Networks

Convolution Pooling Convolution Pooling Fully Fully Output Predictions
+ ReLU + ReLU Connected Connected

dog (0.01)
cat (0.04)

bird (0.02)
o a

s Â£ al C * Pooling = downsampling

* ReLU: Rectified Linear Neurons

Representational Learning

>â€ An 71 44 Layer 1
Ya. Al al E

Fukushima 1980, Krizhevsky 2012]]

RNN - Recurrent Neural Network
â€œOn the Computational Power of Neural Netsâ€ Siegelmamn 1995, Chen 2016
Time unfolding
ht+1
ht
xt
yt
ht+1 = tanh(Wxhxt + Whhht)
Whh
Wxh
yt+1 = g(ht)

[DescripciÃ³n de la imagen: un circuito con un circuito y un circuito con un circuito | TranscripciÃ³n de la imagen: RNN - Recurrent Neural Network

ht+1 â€” tanh(thxÂ¡ + Whhht)

hÂ¡ E | ht+1 Y
w
Wes
xl- -- yÃ x b ' ... "..
Y+1 - 8 (ht) Time unfolding

â€œOn the Computational Power of Neural Netsâ€ Siegelmamn 1995, Chen 2016]]

LSTM - Long Short Term Memory
Hochreiter & Schmidhuber 1997,   Chen 2016
g = tanh(bg + xtUg + htâˆ’1Vg)
i = Ïƒ(bi + xtUi + htâˆ’1Vi)
f = Ïƒ(bf + xtUf + htâˆ’1Vf)
st = stâˆ’1 âˆ˜f + g âˆ˜i
o = Ïƒ(bo + xtUo + htâˆ’1Vo)
ht = tanh(st) âˆ˜o
g âˆ˜i

[DescripciÃ³n de la imagen: lsm - memoria a largo plazo lsm - memoria a largo plazo | TranscripciÃ³n de la imagen: LS | M - Long Short |Term Memory

= f= 0(D +xU +h, ,) : , :
t1 o =|l0o(b + xU +h, ,V)
input * forget
Y gate y Y gate
Xe | f
g = tanh(b? + x U* + h, Â¡V*) h,

Hochreiter â‚¬z Schmidhuber 1997, Chen 2016]]

DBN - Deep Belief Networks
E = âˆ‘
i
WijXY
Hebian Energy Function
Hopfield Neural Network
Stacked Boltzmann Machine
â€œDeep Belief Netsâ€ Hinton 1995

[DescripciÃ³n de la imagen: dn - de - redes hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas hÃ­bridas | TranscripciÃ³n de la imagen: DbN - Deep belief Nerworks

N2>N3
N1>N2

N4>N1

N1>N4

N3>N1 N2>N4
N3>N2 '//'
N3>N4
N4>N3

Hoptield Neural Network

basin of attraction

E=> WiXY

l

Hebian Energy Function

Stacked Boltzmann Machine

â€œDeep Pelief Netsâ€ Hinton 1995]]

Autoencoders
Sparse Autoencoder  
(Kohonnen)
Denoising Autoencoder
Contracting Autoencoder
Dimensionality Reduction
PCA

[DescripciÃ³n de la imagen: hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper hiper]

Variational Autoencoders Generators
https://colab.research.google.com/drive/1Hobi6plfrrUQ9DCiFPZ6vaztOu5Q05ND

[DescripciÃ³n de la imagen: un fondo blanco con un texto rojo que lee, 'vaino autodes generador ']

GAN - Generative Adversarial Networks
Goodfellow 2014
Playing a counterfeit game.
min
G max
D V(D, G) = E[log D(x)] + E[log(1 âˆ’D(G(z))]
G network is trained to increase the chance 
of D assuming and output of G being valid.
D network is trained to decrease the chance 
of assuming G output to be valid.

[DescripciÃ³n de la imagen: gga - genieve aderale networks ga - genieveerale networks ga - genievevere networks ga - gen]

GAN - Generative Adversarial Networks
Mario Klingermann

[DescripciÃ³n de la imagen: un col de la cara de una mujer y un col de la cara de un hombre]

GAN - Generative Adversarial Networks
Ming-Yu Liu et al https://arxiv.org/pdf/1905.01723.pdf

[DescripciÃ³n de la imagen: ga - redes generativas de adversidades | TranscripciÃ³n de la imagen: Ming-Yu Liu et al https: / / arxiv.org/ pdf/1905.01723.pdf]]

Computer Art
https://x.com/quasimondo/status/1706947023037927690?s=20

[DescripciÃ³n de la imagen: arte de la computadora arte de la computadora se llama gennew forma de arte | TranscripciÃ³n de la imagen: Computer Art

EZE COMPUTERWORLD

Computer Art |

; September 11, 1968

s Called

Genuine New Art Form

Computer art, drawn by a plotter under the
direction of a computer, has become some-
thing of a fad this year, and at least one expert
thinks such art represents a genuine new art

The computer has definits potential as an art
medium and should be thoroughly
according to Dr. Thomas S. Fern, chairman of
the Department of Art, University of Notre
Dame. -

https:/ /x.com / quasimondo / status / 1706947023037927690?s=20]]

[DescripciÃ³n de la imagen: una escena de una escena de la pelÃ­cula ala y el rey | TranscripciÃ³n de la imagen: a .
a

l - r â€”
- E â€œ
| - -

b

"
â€” Uâ€”..Mfw.
aA

e â€œ-

-

_
Ll'
AE

,

)Â¿x)]

Transformers
Attention is all you need,  Vaswani 2017

[DescripciÃ³n de la imagen: un diagrama de una transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n transacciÃ³n | TranscripciÃ³n de la imagen: 1ranstormers

TRANSFORMER

s Nx'gÃ±ll

/ ENCODER n DECODERNn Y
ENCODER 3 DECODER 3
ENCODER2 DECODER 2
_ ENCODER 1 DECODER 1

N ENCODER O | DECODER O

N A

INPUT Soy un estudiante â€” :
Attention is all you need, Vaswani 2017]]

Transformers
Attention is all you need,  Vaswani 2017

[DescripciÃ³n de la imagen: transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador transformador | TranscripciÃ³n de la imagen: 1ranstormers

Q
(b,7,d)

Linear

K
(b,n,d)

(b,n,d)
X1| | | | | f "
10 1 1 5 K l

Attention is all you need, Vaswani 2017]]

Diffusion Models
"a bowl of soup that is a portal to another dimension as digital art"
CLIP Contrastive Learning
GLIDE Diffusion Model
Transformer Model

[DescripciÃ³n de la imagen: difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n difusiÃ³n]

Diffusion Models
High-Resolution Image Synthesis With Latent Diffusion Models, Rombach 2022

[DescripciÃ³n de la imagen: modelos de difusiÃ³n alta - sÃ­ntesis de resoluciÃ³n]

Stable Diffusion

[DescripciÃ³n de la imagen: un diagrama de una gasolinera con una gasolinera y una gasolinera | TranscripciÃ³n de la imagen: Stable Diffusion

denoising step crossattention 1witch  skip connection concat]]

Latent Stable Diffusion
U-Net Network conditioned to a 
Condition Embedding.  It reverses the 
diffusion process.
VAE Variational Autoencoder to 
convert to and from the Latent Space.
Condition Encoder CLIP Generate 
Condition Embedding for the UNET

[DescripciÃ³n de la imagen: latente - fecha - fecha latente - fecha latente fecha latente fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha fecha | TranscripciÃ³n de la imagen: Latent Stable Diffusion

Latent Space

to

Diffusion process:

Y
Generate Ãš

Gaussian Noise â€”

T la
E â€œ Ã‰ A

Varational
Autoencoder

(VAE) Attention â€œ â€”
Ir:
UNet Â¡

! _ â€” Update

| I imestep INput

Get Noisy | ME ,

Image at =.
f. l _â€œ

Repeat N times

Condition

Segmentation

r
D
'

Condition

Embedding

| Â¡Condition

Encoder

U-Net Network conditioned to a
Condition Embedding. It reverses the
diffusion process.

VAE Variational Autoencoder to
convert to and from the Latent Space.

Condition Encoder CLIP Generate
Condition Embedding for the UNET]]

Residual Neural Networks and U-Nets
U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger 2015

[DescripciÃ³n de la imagen: Redes neuronales residuales para la segmentaciÃ³n neuronal | TranscripciÃ³n de la imagen: Timestep
Embedding

Residual Neural Networks and U-Nets

â€”

Activation Function

C
h,
1a)
=
J
u
c
o
+-
e
;
<

Activation Function

570 x 570
568 x 5658

' 1283 128

. -
â€”

Taa
u

Ju Â» Y
-- de ?

output
segmentation

map

388 x IRB

m CONV 3x3, ReLU
= copy and crop
$ max pool 2x2
$ up-conv 2x2
=Â» CONV 1x1

U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger 2015]]

Cross Attention
U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger 2015

[DescripciÃ³n de la imagen: acciÃ³n cruzada u - red celular para el segmento biolÃ³gico | TranscripciÃ³n de la imagen: Cross Attention

Q
(b,h*w,d)

Reshape

h_ (b,h*w,c)

Y (b,h*w,)
QK - (b,h*w,d)

softmar ( â€” > v
Vd

V
(b,n ,d)

U-Net: Convolutional Networks ftor Biomedical Image Segmentation, Ronneberger 2015]]

Conditional Encoder
U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger 2015

[DescripciÃ³n de la imagen: un diagrama del sistema funcional funcional | TranscripciÃ³n de la imagen: Conditional Encoder

N
Text Prompts â€” .
v
l a pug gâ€” L ext â€” â€” N

| aÃ±ilinn Ti Ea n
y Encoder ext Prompts ECO
'Âº l a pug _0_l_0_ 0 | N
= â€œ o oo

Cross Entrepy
Across Text Prompts

N Images 2
T, -r,Â¡,11â€” Â£l [ â€¦ . t Cross Entropy
CLI P Probabiliti | Across Images
Image i T, z._1n robabili esrâ€” o | _
, - % : ; , ,

Encoder '

'll']:_Â¡'1Â¡_f'

Backpropagation

U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger 2015]]

Reinforcement Learning
Mnih 2015,Lavet 2018,Sutton 2018

[DescripciÃ³n de la imagen: un diagrama de una palabra o verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo verbo | TranscripciÃ³n de la imagen: Reintorcement Learning

reward observation action

Figure 1.2: The RL problem.

Mnih 2015,Lavet 2018,Sutton 2018]]

Deep Reinforcement Learning
Mnih 2015,Lavet 2018,Sutton 2018
â€¢ Start at s:state, a:action
â€¢ Q(s,a) based on greediest a
â€¢ Iterate
â€¢ Apply a
â€¢ Get sâ€™ 
â€¢ Get r(sâ€™)
â€¢   Q(s, a) = Î±[r + Î³ max
aâ€²ï¿¼Q(sâ€²ï¿¼, aâ€²ï¿¼) âˆ’Q(s, a)]
Q: Deep Neural Network

[DescripciÃ³n de la imagen: Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo aprender Refuerzo profundo | TranscripciÃ³n de la imagen: Deep Remtorcement Learning

* Start at s:state, a:action
* Q(s,a) based on greediest a Â»
* lterate

* Applya

* Gets

* Getr(sâ€)

* 0(5,4) = alr +7 max 0(s', a) â€” 0(5, a)|

Q: Deep Neural Network

Mnih 2015,Lavet 2018,Sutton 2018]]

Are our jobs been displaced by robotics?
Noam Chomsky https://www.youtube.com/watch?v=ojzNM6tRiyc
Income
Dextrous
Cognitive
Plenty of work to be done
Let the robot do the more 
dangerous and boring stuff

[DescripciÃ³n de la imagen: son puestos de trabajo dependientes hous]

Privacidad
HIPAA
GDPR
Deep Fake
Neuroderechos

[DescripciÃ³n de la imagen: pidad hia cpr profundo lago nerosh]

What are current real risks of DL?
Biased
Privacy
Accountability
https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai
DeepFake
Cambridge
Analytical
Digital
Boundary
Lost
Agency
Minority
Report
Hacking 
Autonomous
Car
Is there anything 
unbiased?

[DescripciÃ³n de la imagen: un triÃ¡ngulo rojo con una flecha negra]

Unsettled discussion

[DescripciÃ³n de la imagen: un post de twitter con el capt que dice, 'decision abierta' | TranscripciÃ³n de la imagen: Unsettled discussion

Â©

Rodney Brooks Grodneyabrooks - 12h

Anthony Jules and | explain ORobustAl's new software/hardware combo
Grace and Carter, and how they change the way people can work in
warehouses. Workers can control and interact with the robots whenever
they choose to do so. Collaborative Productivity.

youtube.com
Robust.Al on Collaborative Productivity
At Robust.Al, we make robots work for people.More

| info: https://www.robust.ai/

O 3 T1 16 O 58 ur

Yann LeCun Evylecun - 2h
And your perception system doesn't use deep learning, right?

O 2 ti O s T

Rodney Brooks Grodneyabrooks - 16m

Are you trolling me Sylecun?? As always, | send you respect and sincere
thanks for all that you have done. To answer: one component of our
perception system is multiple model DL inference at frame rates.]]

El pato de Vaucanson
 
Grace Lindsay[47], Guest 2021 [52]
Planes donâ€™t flap their 
wings
Principle of multiple  realizability:
â€œdifferent substrates can nonetheless perform the same input-output
mapping but they are not the sameâ€.

[DescripciÃ³n de la imagen: una estatua de oro con un fondo negro y un fondo rojo]

Functionalism vs Mechanism
if the clockwork clock behaves 
like the digital clock, then the 
clockwork clock is a digital clock

[DescripciÃ³n de la imagen: funchm vs mecanismo el reloj mueve el reloj, luego el reloj se mueve]

The ability to fit
 
Collective Wisdom: 
principles and mechanism [52]

[DescripciÃ³n de la imagen: la capacidad de los cubos]

The ability to summarize
Intelligence
Abstract
Compress
A Theory of Abstraction in RL, David Abel
Del Rigor de la Ciencia, Borges
Complexity of
The World

[DescripciÃ³n de la imagen: la capacidad sumarie]

Connectionism vs Symbolism
Looking back, looking ahead: Symbolism vs connections AI, Goel 2021
System I
System II
Socially Situated
Intelligence
Embodiment

[DescripciÃ³n de la imagen: un diagrama de un sistema con las palabras]

The Age of Data Manifolds

[DescripciÃ³n de la imagen: la antigÃ¼edad de los datos | TranscripciÃ³n de la imagen: 1he Age of Data Manifolds

a â€” Databases
(Cloud / On
Premises)

Cloud Sources

O a
AAzure â€

Audio/
Video/Doacu ments

WebApps,

Sensors, loT ) k Social Feeds

â€”

y Â¿]]

https://www.robust.ai/

[DescripciÃ³n de la imagen: un reproductor de vÃ­deo estÃ¡ reproduciendo un vÃ­deo en un estante | TranscripciÃ³n de la imagen: And now we re building a new iconic class -- Carter.

D Â»i =) 040/4:04 0 E S 4 O

Robust.Al on Collaborative Productivity

https:/ /www.robust.ai /]]

IEEE Spectrum DARPA Challenge

[DescripciÃ³n de la imagen: una motocicleta que estÃ¡ sentada en el suelo]

Is DL++ good or bad?
Good
Bad
make the world more open and connected
Donâ€™t be evil
Internet will provide to everyone, access 
To humanityâ€™s knowledge

[DescripciÃ³n de la imagen: es d + g + od?????????????????????]

Is DL++ good or bad?
Technology
Is
Inherently
Good
Allows more DOF

[DescripciÃ³n de la imagen: es d + g + od? identidad de la tecnologÃ­a buena allover hacer]

Emerging Technologies
â–GANs
â–Explanaible AI
â–Edge AI
â–AI PaaS
â–Adaptive ML
â–Emotion AI

[DescripciÃ³n de la imagen: energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a energÃ­a | TranscripciÃ³n de la imagen: *

Ã“.Q

9
Ã“.Ã“

GANs
Explanaible Al
Edge Al

Al Paas
Adaptive ML

Emotion Al

Emerging l echnologies

Gartner Hype Cycle for
Emerging Technologies, 2019

Expectations

Biochips
AlPass *

Edge Analytics,
Autonomcus Oriving Leve!5 * . y
Low-Esrth-Orbit Satellite Systams..

Fdge Al.. â€” Graph Analytics
Explainable Al â€”
Personification â€” _
Knowiedge Graphs - -;_-7 â€” Next-Generation Memory

Synthetic Data â€”â€” 3D Sensing Cameras

Light Cargo Delivery Drones â€”

Iransfer Leaming â€” _

Flying Autonomous Vehicles â€” â€”
Augmented Intelligence

Nanoscale 3D Printing

â€” Emection Al

â€”

Autoncmous Driving Level 4

Decentrelized Autonomous â€” â€”

Organization â€”DigitalOps
Generalive Adversarial Adaptive ML
Networks â€” â€”
Decentrelized Web â€” â€”
AR Cloud-â€”

â€” Immeresive Workspacee

Biotech - Cultured â€”
or Artificial Tissue

Peak of
Innovation Inflated Trough of Slope of Plateau of
Trigger Expectations Disillusionment Enlightenment Productivity

Time

â€” less than ? years $ ? to5 years S 5to10 yaars C) more than 10 years â€” q bsalste befors plateaa As of August 2018

Pleteau will be resched:]]

Emerging  
Technologies
â–Autonomic Systems
â–Generative Design AI
â–Causal IA
â–Metaverse
â–Web3/Decentralized Identity
â–Data Observability

[DescripciÃ³n de la imagen: un grÃ¡fico con las palabras hyper technologies y el texto hyper technologies | TranscripciÃ³n de la imagen: Â»,
Q.Ã“

*,
Q.Ã“

e,
.

e,
De

Â»,
0.0

O,
Q.Ã“

Emerging
1echnologies

Autonomic Systems
Generative Design Al

Causal IA

Metaverse

Web3 / Decentralized Identity
Data Observability

Hype Cycle for Emerging Tech, 2022

Expectations

Soures: Sartner
6 2027 Cartner, Inc. andjor its atilates All rights reserved, Cartner and Hype Cycls ere registered trademarks of Cartner, Inc. andits affiliates n +he L.S, 1893703

Foundation Models
Computational S:orage .
Superapps â€”
Industry Cioud Platforms â€”

-Decentralizec Identity
â€” NFT
â€”Cloud Data Ecosystems

Internal Talent Markstplaces â€”

Digira Humans

Dynamic Risk

Governance
Cbservability-Driven

Development -

Data Observability Clouc Sustainab liy

Platrform Engineering -
Causal Al â€¦|X Metaverse
Open Teleme:ry â€” |
Minimum Viab e
Architecture â€œ
Digital "wir of
a Customer

â€”Augmented FinOps
Mach ne _earning
Coce Generation

â€”Cenerative Design Al

Cybersecurity Autonomic Systems

Mesh Innovation

Architecture TfÂ¡gger

Peak of
Inflated
Expectations

Plateau of
Productivity

Troughof
Disillusionment

Slope of
Enlightenment

Time

Plateau will be reached:

() less tnan 2 years O 2tu5 years $ 51010 years Ã More than 10 years 0) Obsolete before plateau As of August 2022

gartner.com

Gartner]]

Applied AI
Liveness Detection
DS
Retail 
OCR de los Telegrama Electorales 
Estado de la fruta
Seguros de Exportaciones
Customer Satisfaction
NLP

[DescripciÃ³n de la imagen: AplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a la aplicaciÃ³n a]

Connectionism vs Symbolism

[DescripciÃ³n de la imagen: el individuo frente al colectivo | TranscripciÃ³n de la imagen: The Individual versus the Collective

* And then came LLMs...

* Suddenly those large, distributed collections of data and functions and
gradients seemed to be acting human-like and have a â€œmindâ€â€”the phrase
â€œAlâ€ was retrieved â€”and the focus again was on an intelligent agent, for no
particularly good reason

E ACTIO "M
$$$$$$

CONFERE N|
A!l, Science and Society]]

Fallacies of Data Science
â–Biased Cherry Picking: Selecting results that fit your claim.
â–Data Dredging: Results of correlation was due to chance.
â–Survivorship bias: Drawing conclusions from an incomplete set of data that survived.
â–Perverse incentive: the incentive produces the opposite result (wrong metrics, KPIs).
â–False causality: two things occur together, they are related (falsely)
â–Garbage in, garbage out.
â–Sampling bias (cross validation, bootstrapping)
â–Gamblers fallacy: believing that when something happen the chance of happening again is 
reduced.
â–Unbalanced Datasets: medical images
â–Simpsons paradox: a trend is present in a group but disappears when data is combined.
â–Clever Hans Effect: https://arxiv.org/pdf/2006.10609.pdf
ï¿½ã¤ â¡ï¿½ã¤â¡ï¿½ã¤

[DescripciÃ³n de la imagen: una casa con una bandera y un hombre de pie delante de ella]

Fallacies of Data Science

[DescripciÃ³n de la imagen: un pedazo de papel con las palabras "falso" y "falso" | TranscripciÃ³n de la imagen: Fallacies of Data Science

It also muddies the origin of certain data sets. This can mean that

researchers miss important features that skew the training of their models.

In yet other cases, som to be picking up on the text font
ertain hospitals used to label the scans. As a result, fonts from hospitals

with more serious caseloads became predictors of covid risk.]]

Machine Learning
https://twitter.com/vardi/status/1566070939586027520?s=20&t=MHwWjzTDfRfm2d4emgIImg

[DescripciÃ³n de la imagen: machine learning lo que es machine learning | TranscripciÃ³n de la imagen: Machine Learning

Math break!
1 .
What number will be next?

- (CJ e , O
N Y

-
â€”

v 1
!gÂ¡; 217341, because if
f(x) = 18111/2-x"4-90555-x"3+6
33885/2-x12-452773*+x+217331,

https://twitter.com/vardi/status/1566070939586027520?s=20 Â£t=MHwW1Z TDfRfm2d4emglImg]]

Artificial Intelligence
https://twitter.com/ExcelHumor/status/1589098712177217538?s=20&t=w97fO1jJWtTOw8R5kLwJmQ

[DescripciÃ³n de la imagen: un personaje de dibujos animados con una camisa blanca y corbata naranja | TranscripciÃ³n de la imagen: Arutificial Intelligence

https://twitter.com/ExcelHumor/status/1589098712177217538?s=20<t=w97fO01)IWtTOw8RS5kLwJmQ]]

The age of Data Manifolds
Manifold Learning: Model Reduction in Engineering, Ryckelynck

[DescripciÃ³n de la imagen: el diagrama del flujo de datos en el flujo de datos | TranscripciÃ³n de la imagen: 1he age of Data Manitolds

a â€” Databases
(Cloud / Ã“n
" Premises)

Cloud Sources

O a

Audio/
Video/Documents
14 Azure
â€” â€” Â» E
N AA , [
WebApps, '

Sensors, loT Social Feeds
. K
a Â¿ I f

y E
%'%3 )

y Â¿

Manifold Learning: Model Reduction in Engineering, Ryckelynck]]

2050
2025
2075

[DescripciÃ³n de la imagen: un col de diferentes imÃ¡genes y texto]

En resumen
 
Sutton
http://www.incompleteideas.net/IncIdeas/BitterLesson.html
Rodney Brooks
https://rodneybrooks.com/a-better-lesson/

[DescripciÃ³n de la imagen: un fondo blanco con un texto rojo que lee f equinn | TranscripciÃ³n de la imagen: En resumen

Sutton

http:/ /www.incompleteideas.net/ Incldeas / BitterLesson.html

Rodney Brooks

https: / / rodnevbrooks.com / a-better-lesson]]

The bitter lesson

[DescripciÃ³n de la imagen: la amarga lecciÃ³n | TranscripciÃ³n de la imagen: 1he bitter lesson

U

.+1 1
.]]

En resumen
 
IEEE Spectrum March 2020

[DescripciÃ³n de la imagen: la portada del libro enum]

En resumen
â–Focalizado para representar datos que intrÃ­nsecamente estÃ¡nÂ estructurados jerÃ¡rquicamente.
â–Las capas intermedias ofrecen contenido que tienenÂ valor semÃ¡ntico (es como una salida intermedia parcial de la 
red).Â  E.g.: la primer capa me separa arriba y abajo y la segunda derecha e izquierda.
â–El "deep" implica que la cantidad de saltos en la dependencia causal es mayor.
â–Estos layersÂ adicionales permiten que los features los genere y encuentre la misma red (en vez de ponÃ©rselos a 
mano).Â  La red hace la reducciÃ³n de la dimensionalidad.
â–Algunos ajustes a los algoritmos de arquitectura y aprendizaje que aprovechan estas arquitecturas y el 
paralelismo.
â–No estarÃ­amos hablando de esto sin GPUs y sin MooreÂ´s Law.
â–Extraordinarios resultados en tres Ã¡reas particulares: image processing, speech recognition y natural language 
processing.
â–Excelentes para implementaciones Cheery-Picking en escenarios donde no existe actualmente DigitalizaciÃ³n.
â–El componente estocÃ¡stico que tienen las redes hace muy complejo su parametrizaciÃ³n.
â–Limitaciones cuando la cantidad de muestras es baja[26].
â–Inteligible Property[25]
â–Hyperhype:  Through of Disillusionment (Gartner Hype Cycle)

[DescripciÃ³n de la imagen: prosewnn]

ã©ã†ã‚‚ã‚ã‚ŠãŒã¨ã†â€¨
Gracias !
Centro Inteligencia Computacional  
ITBA

[DescripciÃ³n de la imagen: un personaje de dibujos animados con una cara verde y un pelo negro]

Follow up
â–Tutorials at Conferences (e.g. ICML, NeurIPS)
â–Two Minute Papers - Youtube Channel
â–The Batch - Andrew NG newsletter
â–Yannic Kilcher - Youtube Channel
â–Arxiv Insight - Youtube Channel
â–http://monostuff.logdown.com/posts/7835544-aprendizaje-modo-mquina

[DescripciÃ³n de la imagen: @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ | TranscripciÃ³n de la imagen: Follow up

* Tutorials at Conferences (e.g. ICML, NeurlPS)
* Two Minute Papers - Youtube Channel

* The Batch - Andrew NG newsletter

* Yannic Kilcher - Youtube Channel

â€œ Arxiv Ins1ght Youtube Channel]]

[1] Alberts, Bruce, et al.Â Essential cell biology. Garland Science, 2013. 
[2] Guyton and Hall, â€œTextbook of Medical Physiologyâ€, 12th edition  
[3] Izhikevich, â€œDynamical systems in neuroscienceÂ·Â·, 1st edition  
[4] Abott & Dayan, Theoretical Neuroscience, 2012 
[5] Ramele, R., A. J. Villar, and J. M. Santos. "A Brain Computer Interface Classification Method Based on Signal Plots." 4th Winter Conference 
on Brain Computer Interfaces, Yongpyong, Korea, February 2015. IEEE Signal and Processing, 2016.  
[6] Chia-Chu et al, â€œSlow periodic activity in the longitudinal hippocampal slice can selfâ€propagate nonâ€synaptically by a mechanism consistent 
with ephaptic couplingâ€, The Physiological Society Journal, 2018 
[7] Belousov et al, â€œNeuronal gap junctions: making and breaking connections during development and injuryâ€, Trends in Neuroscience, 2012 
[8] Khraiche, â€œUltrasound induced increase in excitability of single neuronsâ€, 2008 
[9] Szegedy, â€œGoing deeper with convolutionsâ€, 2014 https://arxiv.org/abs/1409.4842 
[10] Montufar, â€œOn the Number of Linear Regions of Deep Neural Networksâ€, 2014 
[11] Graves, â€œNeural Turing Machinesâ€, 2014 
[12] Schmidhuber, â€œDeep learning in neural networks: An overviewâ€, 2015 
[13] Goodfellow, â€œDeep Learningâ€, 2017 
[14] Sutton, Richard S., and Andrew G. Barto.Â Reinforcement learning: An introduction. MIT press, 2018.

[DescripciÃ³n de la imagen: la bibliografÃ­a de la universidad de tecnologÃ­a e ingenierÃ­a]

[15] http://www.dfki.de/~klusch/DeepLearning-seminar-ss17/Topics/topics.html 
[16] Domingos, A Few Useful Things to Know about Machine Learning, 2015 
[17] http://deeplearning.net/ 
[18] Krizhevsky, â€œImageNet Classification with Deep Convolutional Neural Networksâ€, 2012 
[19] https://www.acm.org/media-center/2019/march/turing-award-2018 
[20] https://www.jeremyjordan.me/autoencoders/ 
[21] MNIST Dataset http://yann.lecun.com/exdb/mnist/ 
[22] IRIS Dataset https://archive.ics.uci.edu/ml/datasets/Iris 
[23] AUTO MPG Dataset https://archive.ics.uci.edu/ml/datasets/Auto+MPG 
[24] Kaggle Datasets 
[25] M Bragg, â€œThe Challenge of Crafting Intelligible Intelligenceâ€, ACM Symposium on User Interface Software and Technology, 2018.  
[26] F. Lotte et al, â€œA review of classification algorithms for EEG-based brain-computer interfaces: A 10 year updateâ€, Journal of Neural 
Engineering 15 (2018), no. 3, 031005.  
[27] https://medium.freecodecamp.org/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159 
[28] Goodfellow, â€œGenerative Adversarial Netsâ€, 2014 
[29] Coral Google Dev Board https://coral.withgoogle.com/products/dev-board/ 
[30] Chollet, Deep Learning with Python, 2018

[DescripciÃ³n de la imagen: un papel blanco con un tÃ­tulo blanco y negro]

[31] Jetson TK1 Dev https://www.nvidia.com/object/jetson-tk1-embedded-dev-kit.html 
[32] Langr, GANs in action, Manning 2019 
[33] Reitz, The Hitchhikerâ€™s Guide to Python!, 2016 
[34] A Few Useful Things to Know about Machine Learning, Pedro Domingos, ACM 2010 
[35] Data Mining and Analysis, Mohammed J.Zaki, 2010 
[36] An Introduction to Multivariate Analysis, Everit ,2012 
[37] https://adventuresinmachinelearning.com/keras-lstm-tutorial/ 
[38] https://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html 
[39] Ben Simon, The dark side of the alpha rhythm: FMRI evidence for induced alpha modulation during complete darkness, 2012 
[40] https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b 
[41] https://www.tensorflow.org/swift 
[42] Gaussia Process https://yugeten.github.io/posts/2019/09/GP/ 
[43] Hopfield J.J, Computing with neural circuits: a model, 1986 
[40] Anthony Zador, A critique of pure learning and what artificial neural networks can learn from animal brains, Nature Communications 2019 
[41] https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/ 
[42] Chollet, On the Measure of Intelligence

[DescripciÃ³n de la imagen: un papel blanco con un tÃ­tulo blanco y negro]

[43] Cybenko, G. (1989) Universal Approximation Theorem 
[44] Sara Hooker, The Hardware Lottery, 2020, https://arxiv.org/abs/2009.06489 
[45] https://blog.acolyer.org/2016/04/18/deep-learning-in-neural-networks-an-overview 
[46] Thompson, R. F. & Kim, J. J. Memory systems in the brain and localization of a memory.Â Proc. Natl. Acad. Sci. U. S. A.Â 93, 13438â€“44 
(1996). 
[47] Grace Lindsay, https://aeon.co/ideas/planes-dont-flap-their-wings-does-ai-work-like-a-brain 
[48] Jordan Guerguiev, Towards deep learning with segregated dendrites, 2017 
[49] Melanie Mitchell, Why AI is harder than we think?, 2021, https://arxiv.org/abs/2104.12871 
[50] Hinton, â€œA Fast Learning Algorithm for Deep Belief Netsâ€, Neural Computation, 2006 
[51] https://medium.com/@shridhar743/a-beginners-guide-to-deep-learning-5ee814cf7706 
[52] Guest, â€œOn logical inference over brains, behaviour, and artificial neural networksâ€, 2021 
[53] http://myselph.de/hodgkinHuxley.html 
[54] https://twitter.com/slava__bobrov/status/1454338973606850563?s=20 
[55] https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction 
[56] https://www.assemblyai.com/blog/how-dall-e-2-actually-works/ 
[57] AlphaZero for Matriz Multiplication https://www.nature.com/articles/s41586-022-05172-4 
https://www.youtube.com/watch?v=AQrMWH8aC0Q&t=4s 
[58] Berghel, Hal. "Malice domestic: The Cambridge analytica dystopia."Â ComputerÂ 51.05 (2018): 84-89. 
[59] Sejinowski, The unreasonable effectiveness of deep learning in artificial intelligence (2019) 
[60] https://slate.com/technology/2014/05/phineas-gage-neuroscience-case-true-story-of-famous-frontal-lobe-patient-is-better-than-textbook-
accounts.html

[DescripciÃ³n de la imagen: un documento con el tÃ­tulo y tÃ­tulo]

