#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de prueba para el sistema RAG usando configuraciÃ³n JSON
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

# Configurar codificaciÃ³n UTF-8 para Windows
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# Agregar el directorio src al path
sys.path.append('src')

# Configurar logging con codificaciÃ³n UTF-8
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def load_test_config(config_file: str = "test_config.json") -> Dict[str, Any]:
    """
    Carga la configuraciÃ³n de pruebas desde un archivo JSON
    
    Args:
        config_file: Ruta al archivo de configuraciÃ³n
        
    Returns:
        Diccionario con la configuraciÃ³n
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ Error: No se encontrÃ³ el archivo de configuraciÃ³n {config_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Error: JSON invÃ¡lido en {config_file}: {e}")
        sys.exit(1)

def run_single_test(rag_pipeline, test_config: Dict[str, Any], config_index: int) -> bool:
    """
    Ejecuta una sola prueba de configuraciÃ³n
    
    Args:
        rag_pipeline: Instancia del pipeline RAG
        test_config: ConfiguraciÃ³n de la prueba
        config_index: Ãndice de la configuraciÃ³n
        
    Returns:
        True si la prueba fue exitosa, False en caso contrario
    """
    print(f"\n{'='*80}")
    print(f"ğŸ§ª PRUEBA {config_index + 1}: {test_config['name']}")
    print(f"ğŸ“ {test_config['description']}")
    print(f"{'='*80}")
    
    try:
        # Obtener parÃ¡metros de la configuraciÃ³n
        query_params = test_config['query_params']
        search_query = test_config.get('search_query', '')
        k_context = test_config.get('k_context', 5)
        
        print(f"\nğŸ¯ ParÃ¡metros de consulta:")
        for key, value in query_params.items():
            print(f"   - {key}: {value}")
        
        print(f"\nğŸ” Consulta de bÃºsqueda: '{search_query}'")
        print(f"ğŸ“š Chunks de contexto: {k_context}")
        
        # Generar ejercicio
        print(f"\nğŸ”„ Generando ejercicio...")
        result = rag_pipeline.generate_exercises(
            query_params=query_params,
            k_retrieval=k_context,
            use_filters=True  # Habilitar filtros ahora que la metadata estÃ¡ correcta
        )
        
        if result.get('status') == 'error':
            print(f"   âŒ Error: {result.get('message', 'Error desconocido')}")
            return False
        
        # Mostrar contexto si estÃ¡ habilitado
        if 'context_info' in result:
            context_info = result['context_info']
            print(f"\nğŸ“š Contexto utilizado:")
            print(f"   - Documentos recuperados: {context_info.get('documents_retrieved', 'N/A')}")
            print(f"   - Consulta de bÃºsqueda: {context_info.get('search_query', 'N/A')}")
            print(f"   - Filtros aplicados: {context_info.get('filters_applied', 'N/A')}")
        
        # Mostrar ejercicio generado
        if 'ejercicios' in result and result['ejercicios']:
            print(f"\nâœ… Ejercicio generado exitosamente!")
            ejercicio = result['ejercicios'][0]
            tipo_ejercicio = query_params.get('tipo_ejercicio', 'unknown')
            
            print(f"\nğŸ“ EJERCICIO DE {tipo_ejercicio.upper()} GENERADO:")
            print("   " + "="*60)
            
            # Mostrar campos segÃºn el tipo de ejercicio
            if tipo_ejercicio == 'desarrollo':
                print(f"   ğŸ“‹ Enunciado: {ejercicio.get('enunciado', 'N/A')}")
                print(f"   ğŸ¯ Objetivos: {ejercicio.get('objetivos', 'N/A')}")
                print(f"   ğŸ“ Instrucciones: {ejercicio.get('instrucciones', 'N/A')}")
                print(f"   ğŸ’¡ Criterios de evaluaciÃ³n: {ejercicio.get('criterios_evaluacion', 'N/A')}")
                print(f"   ğŸ”§ SoluciÃ³n sugerida: {ejercicio.get('solucion_sugerida', 'N/A')}")
                print(f"   ğŸ“š Referencias: {ejercicio.get('referencias', 'N/A')}")
            elif tipo_ejercicio == 'multiple_choice':
                print(f"   ğŸ“‹ Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   ğŸ”¤ Opciones:")
                for i, opcion in enumerate(ejercicio.get('opciones', []), 1):
                    print(f"      {chr(64+i)}. {opcion}")
                print(f"   âœ… Respuesta correcta: {ejercicio.get('respuesta_correcta', 'N/A')}")
                print(f"   ğŸ’¡ Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   ğŸ”§ SoluciÃ³n: {ejercicio.get('solucion', 'N/A')}")
            elif tipo_ejercicio == 'practico':
                print(f"   ğŸ“‹ Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   ğŸ“Š Datos: {ejercicio.get('datos', 'N/A')}")
                print(f"   ğŸ’¡ Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   ğŸ”§ SoluciÃ³n: {ejercicio.get('solucion', 'N/A')}")
            elif tipo_ejercicio == 'teorico':
                print(f"   ğŸ“‹ Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   ğŸ”‘ Conceptos clave: {ejercicio.get('conceptos_clave', 'N/A')}")
                print(f"   ğŸ’¡ Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   ğŸ”§ SoluciÃ³n: {ejercicio.get('solucion', 'N/A')}")
            else:
                # Mostrar todos los campos disponibles
                for key, value in ejercicio.items():
                    print(f"   {key}: {value}")
            
            print("   " + "="*60)
            
            # Mostrar metadata si estÃ¡ habilitado
            if 'metadata' in result:
                print(f"\nğŸ“Š Metadata del ejercicio:")
                metadata = result['metadata']
                print(f"   - Modelo usado: {metadata.get('modelo_usado', 'N/A')}")
                print(f"   - Chunks recuperados: {metadata.get('chunks_recuperados', 'N/A')}")
                print(f"   - Fuentes: {len(metadata.get('fuentes', []))} documentos")
                print(f"   - Materia: {metadata.get('materia', 'N/A')}")
                print(f"   - Unidad: {metadata.get('unidad', 'N/A')}")
                print(f"   - Dificultad: {metadata.get('nivel_dificultad', 'N/A')}")
        else:
            print(f"   âŒ No se generaron ejercicios")
            return False
        
        return True
        
    except Exception as e:
        print(f"   âŒ Error en la prueba: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """FunciÃ³n principal"""
    print("ğŸš€ INICIANDO PRUEBAS DEL SISTEMA RAG")
    print("=" * 80)
    
    # Cargar configuraciÃ³n
    print("ğŸ“‹ Cargando configuraciÃ³n de pruebas...")
    config = load_test_config()
    test_configs = config.get('test_configs', [])
    default_config = config.get('default_config', {})
    
    if not test_configs:
        print("âŒ Error: No se encontraron configuraciones de prueba")
        return
    
    print(f"âœ… ConfiguraciÃ³n cargada: {len(test_configs)} pruebas configuradas")
    
    # Mostrar pruebas disponibles
    print(f"\nğŸ“‹ Pruebas disponibles:")
    for i, test_config in enumerate(test_configs, 1):
        print(f"   {i}. {test_config['name']} ({test_config['query_params']['tipo_ejercicio']})")
    
    # Verificar argumentos de lÃ­nea de comandos
    test_indices = []
    if len(sys.argv) > 1:
        try:
            if sys.argv[1] == "all":
                test_indices = list(range(len(test_configs)))
            else:
                test_indices = [int(arg) - 1 for arg in sys.argv[1:]]
                # Validar Ã­ndices
                invalid_indices = [i for i in test_indices if i < 0 or i >= len(test_configs)]
                if invalid_indices:
                    print(f"âŒ Error: Ãndices invÃ¡lidos: {[i+1 for i in invalid_indices]}")
                    print(f"   Ãndices vÃ¡lidos: 1-{len(test_configs)}")
                    return
        except ValueError:
            print("âŒ Error: Los argumentos deben ser nÃºmeros o 'all'")
            print("Uso: python test_rag.py [all|1|2|3|...]")
            return
    else:
        # Por defecto, ejecutar solo la primera prueba
        test_indices = [0]
    
    print(f"ğŸ¯ Ejecutando pruebas: {[i+1 for i in test_indices]}")
    
    # Inicializar pipeline RAG
    print(f"\nğŸ”§ Inicializando pipeline RAG...")
    try:
        from src.rag_pipeline import RAGPipeline
        
        # Forzar recarga de documentos con metadata correcta
        rag_pipeline = RAGPipeline(reset_collection=True)
        print("âœ… Pipeline RAG inicializado correctamente")
        
        # Verificar si hay documentos cargados
        collection_info = rag_pipeline.vector_store.get_collection_info()
        doc_count = collection_info.get('document_count', 0)
        
        if doc_count == 0:
            print("ğŸ“š No hay documentos cargados. Cargando documentos...")
            result = rag_pipeline.load_materials(
                data_directory="docs",
                file_extensions=[".pdf", ".txt", ".md"]
            )
            
            if result.get('status') == 'success':
                print(f"âœ… Documentos cargados: {result.get('documents_loaded', 0)}")
                print(f"âœ… Chunks creados: {result.get('chunks_created', 0)}")
            else:
                print(f"âŒ Error cargando documentos: {result.get('message', 'Error desconocido')}")
                return
        else:
            print(f"âœ… ColecciÃ³n existente con {doc_count} documentos")
        
    except Exception as e:
        print(f"âŒ Error inicializando pipeline: {str(e)}")
        return
    
    # Ejecutar pruebas
    print(f"\nğŸ§ª INICIANDO PRUEBAS")
    print("=" * 80)
    
    successful_tests = 0
    total_tests = len(test_indices)
    
    for i, test_index in enumerate(test_indices):
        test_config = test_configs[test_index]
        success = run_single_test(rag_pipeline, test_config, test_index)
        if success:
            successful_tests += 1
    
    # Resumen final
    print(f"\n{'='*80}")
    print(f"ğŸ“Š RESUMEN DE PRUEBAS")
    print(f"{'='*80}")
    print(f"âœ… Pruebas exitosas: {successful_tests}/{total_tests}")
    print(f"âŒ Pruebas fallidas: {total_tests - successful_tests}/{total_tests}")
    print(f"ğŸ“ˆ Tasa de Ã©xito: {(successful_tests/total_tests)*100:.1f}%")
    
    if successful_tests == total_tests:
        print(f"\nğŸ‰ Â¡Todas las pruebas pasaron exitosamente!")
    else:
        print(f"\nâš ï¸ Algunas pruebas fallaron. Revisa los logs para mÃ¡s detalles.")
    
    print(f"\nğŸ’¡ Uso del script:")
    print(f"   python test_rag.py          # Ejecutar primera prueba")
    print(f"   python test_rag.py all      # Ejecutar todas las pruebas")
    print(f"   python test_rag.py 1 3 5    # Ejecutar pruebas especÃ­ficas")

if __name__ == "__main__":
    main()