#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de prueba para el sistema RAG usando configuraci√≥n JSON
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

# Configurar codificaci√≥n UTF-8 para Windows
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# Agregar el directorio src al path
sys.path.append('src')

# Configurar logging con codificaci√≥n UTF-8
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def load_test_config(config_file: str = "test_config.json") -> Dict[str, Any]:
    """
    Carga la configuraci√≥n de pruebas desde un archivo JSON
    
    Args:
        config_file: Ruta al archivo de configuraci√≥n
        
    Returns:
        Diccionario con la configuraci√≥n
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"‚ùå Error: No se encontr√≥ el archivo de configuraci√≥n {config_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: JSON inv√°lido en {config_file}: {e}")
        sys.exit(1)

def run_single_test(rag_pipeline, test_config: Dict[str, Any], config_index: int) -> bool:
    """
    Ejecuta una sola prueba de configuraci√≥n
    
    Args:
        rag_pipeline: Instancia del pipeline RAG
        test_config: Configuraci√≥n de la prueba
        config_index: √çndice de la configuraci√≥n
        
    Returns:
        True si la prueba fue exitosa, False en caso contrario
    """
    print(f"\n{'='*80}")
    print(f"üß™ PRUEBA {config_index + 1}: {test_config['name']}")
    print(f"üìù {test_config['description']}")
    print(f"{'='*80}")
    
    try:
        # Obtener par√°metros de la configuraci√≥n
        query_params = test_config['query_params']
        search_query = test_config.get('search_query', '')
        k_context = test_config.get('k_context', 5)
        
        print(f"\nüéØ Par√°metros de consulta:")
        for key, value in query_params.items():
            print(f"   - {key}: {value}")
        
        print(f"\nüîç Consulta de b√∫squeda: '{search_query}'")
        print(f"üìö Chunks de contexto: {k_context}")
        
        # Generar ejercicio
        print(f"\nüîÑ Generando ejercicio...")
        result = rag_pipeline.generate_exercises(
            query_params=query_params,
            k_retrieval=k_context,
            use_filters=True  # Habilitar filtros ahora que la metadata est√° correcta
        )
        
        if result.get('status') == 'error':
            print(f"   ‚ùå Error: {result.get('message', 'Error desconocido')}")
            return False
        
        # Mostrar contexto si est√° habilitado
        if 'context_info' in result:
            context_info = result['context_info']
            print(f"\nüìö Contexto utilizado:")
            print(f"   - Documentos recuperados: {context_info.get('documents_retrieved', 'N/A')}")
            print(f"   - Consulta de b√∫squeda: {context_info.get('search_query', 'N/A')}")
            print(f"   - Filtros aplicados: {context_info.get('filters_applied', 'N/A')}")
        
        # Mostrar ejercicio generado
        if 'ejercicios' in result and result['ejercicios']:
            print(f"\n‚úÖ Ejercicio generado exitosamente!")
            ejercicio = result['ejercicios'][0]
            tipo_ejercicio = query_params.get('tipo_ejercicio', 'unknown')
            
            print(f"\nüìù EJERCICIO DE {tipo_ejercicio.upper()} GENERADO:")
            print("   " + "="*60)
            
            # Mostrar campos seg√∫n el tipo de ejercicio
            if tipo_ejercicio == 'desarrollo':
                print(f"   üìã Enunciado: {ejercicio.get('enunciado', 'N/A')}")
                print(f"   üéØ Objetivos: {ejercicio.get('objetivos', 'N/A')}")
                print(f"   üìù Instrucciones: {ejercicio.get('instrucciones', 'N/A')}")
                print(f"   üí° Criterios de evaluaci√≥n: {ejercicio.get('criterios_evaluacion', 'N/A')}")
                print(f"   üîß Soluci√≥n sugerida: {ejercicio.get('solucion_sugerida', 'N/A')}")
                print(f"   üìö Referencias: {ejercicio.get('referencias', 'N/A')}")
            elif tipo_ejercicio == 'multiple_choice':
                print(f"   üìã Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   üî§ Opciones:")
                for i, opcion in enumerate(ejercicio.get('opciones', []), 1):
                    print(f"      {chr(64+i)}. {opcion}")
                print(f"   ‚úÖ Respuesta correcta: {ejercicio.get('respuesta_correcta', 'N/A')}")
                print(f"   üí° Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   üîß Soluci√≥n: {ejercicio.get('solucion', 'N/A')}")
            elif tipo_ejercicio == 'practico':
                print(f"   üìã Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   üìä Datos: {ejercicio.get('datos', 'N/A')}")
                print(f"   üí° Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   üîß Soluci√≥n: {ejercicio.get('solucion', 'N/A')}")
            elif tipo_ejercicio == 'teorico':
                print(f"   üìã Pregunta: {ejercicio.get('pregunta', 'N/A')}")
                print(f"   üîë Conceptos clave: {ejercicio.get('conceptos_clave', 'N/A')}")
                print(f"   üí° Pista: {ejercicio.get('pista', 'N/A')}")
                print(f"   üîß Soluci√≥n: {ejercicio.get('solucion', 'N/A')}")
            else:
                # Mostrar todos los campos disponibles
                for key, value in ejercicio.items():
                    print(f"   {key}: {value}")
            
            print("   " + "="*60)
            
            # Mostrar metadata si est√° habilitado
            if 'metadata' in result:
                print(f"\nüìä Metadata del ejercicio:")
                metadata = result['metadata']
                print(f"   - Modelo usado: {metadata.get('modelo_usado', 'N/A')}")
                print(f"   - Chunks recuperados: {metadata.get('chunks_recuperados', 'N/A')}")
                print(f"   - Fuentes: {len(metadata.get('fuentes', []))} documentos")
                print(f"   - Materia: {metadata.get('materia', 'N/A')}")
                print(f"   - Unidad: {metadata.get('unidad', 'N/A')}")
                print(f"   - Dificultad: {metadata.get('nivel_dificultad', 'N/A')}")
        else:
            print(f"   ‚ùå No se generaron ejercicios")
            return False
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Error en la prueba: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Funci√≥n principal"""
    print("üöÄ INICIANDO PRUEBAS DEL SISTEMA RAG")
    print("=" * 80)
    
    # Cargar configuraci√≥n
    print("üìã Cargando configuraci√≥n de pruebas...")
    config = load_test_config()
    test_configs = config.get('test_configs', [])
    default_config = config.get('default_config', {})
    
    if not test_configs:
        print("‚ùå Error: No se encontraron configuraciones de prueba")
        return
    
    print(f"‚úÖ Configuraci√≥n cargada: {len(test_configs)} pruebas configuradas")
    
    # Mostrar pruebas disponibles
    print(f"\nüìã Pruebas disponibles:")
    for i, test_config in enumerate(test_configs, 1):
        print(f"   {i}. {test_config['name']} ({test_config['query_params']['tipo_ejercicio']})")
    
    # Verificar argumentos de l√≠nea de comandos
    test_indices = []
    if len(sys.argv) > 1:
        try:
            if sys.argv[1] == "all":
                test_indices = list(range(len(test_configs)))
            else:
                test_indices = [int(arg) - 1 for arg in sys.argv[1:]]
                # Validar √≠ndices
                invalid_indices = [i for i in test_indices if i < 0 or i >= len(test_configs)]
                if invalid_indices:
                    print(f"‚ùå Error: √çndices inv√°lidos: {[i+1 for i in invalid_indices]}")
                    print(f"   √çndices v√°lidos: 1-{len(test_configs)}")
                    return
        except ValueError:
            print("‚ùå Error: Los argumentos deben ser n√∫meros o 'all'")
            print("Uso: python test_rag.py [all|1|2|3|...]")
            return
    else:
        # Por defecto, ejecutar solo la primera prueba
        test_indices = [0]
    
    print(f"üéØ Ejecutando pruebas: {[i+1 for i in test_indices]}")
    
    # Inicializar pipeline RAG
    print(f"\nüîß Inicializando pipeline RAG...")
    try:
        from src.rag_pipeline import RAGPipeline
        
        # Forzar recarga de documentos con metadata correcta
        rag_pipeline = RAGPipeline(reset_collection=True)
        print("‚úÖ Pipeline RAG inicializado correctamente")
        
        # Verificar si hay documentos cargados
        collection_info = rag_pipeline.vector_store.get_collection_info()
        doc_count = collection_info.get('document_count', 0)
        
        if doc_count == 0:
            print("üìö No hay documentos cargados. Cargando documentos...")
            result = rag_pipeline.load_materials(
                data_directory="docs",
                file_extensions=[".pdf", ".txt", ".md"]
            )
            
            if result.get('status') == 'success':
                print(f"‚úÖ Documentos cargados: {result.get('documents_loaded', 0)}")
                print(f"‚úÖ Chunks creados: {result.get('chunks_created', 0)}")
            else:
                print(f"‚ùå Error cargando documentos: {result.get('message', 'Error desconocido')}")
                return
        else:
            print(f"‚úÖ Colecci√≥n existente con {doc_count} documentos")
        
    except Exception as e:
        print(f"‚ùå Error inicializando pipeline: {str(e)}")
        return
    
    # Ejecutar pruebas
    print(f"\nüß™ INICIANDO PRUEBAS")
    print("=" * 80)
    
    successful_tests = 0
    total_tests = len(test_indices)
    
    for i, test_index in enumerate(test_indices):
        test_config = test_configs[test_index]
        success = run_single_test(rag_pipeline, test_config, test_index)
        if success:
            successful_tests += 1
    
    # Resumen final
    print(f"\n{'='*80}")
    print(f"üìä RESUMEN DE PRUEBAS")
    print(f"{'='*80}")
    print(f"‚úÖ Pruebas exitosas: {successful_tests}/{total_tests}")
    print(f"‚ùå Pruebas fallidas: {total_tests - successful_tests}/{total_tests}")
    print(f"üìà Tasa de √©xito: {(successful_tests/total_tests)*100:.1f}%")
    
    if successful_tests == total_tests:
        print(f"\nüéâ ¬°Todas las pruebas pasaron exitosamente!")
    else:
        print(f"\n‚ö†Ô∏è Algunas pruebas fallaron. Revisa los logs para m√°s detalles.")
    
    print(f"\nüí° Uso del script:")
    print(f"   python test_rag.py          # Ejecutar primera prueba")
    print(f"   python test_rag.py all      # Ejecutar todas las pruebas")
    print(f"   python test_rag.py 1 3 5    # Ejecutar pruebas espec√≠ficas")

if __name__ == "__main__":
    main()