# GuÃ­a de EvaluaciÃ³n con RAGAS

## ğŸ¯ Script de EvaluaciÃ³n: `evaluate_with_ragas.py`

Este script evalÃºa tu sistema RAG usando las mÃ©tricas de RAGAS:

### MÃ©tricas Evaluadas

#### âœ… Sin Ground Truth (Respuestas Correctas):
1. **Faithfulness (Fidelidad)** - Â¿La respuesta es fiel al contexto recuperado?
2. **Answer Relevance (Relevancia)** - Â¿La respuesta es relevante a la pregunta?

#### âœ… Con Ground Truth:
3. **Context Precision (PrecisiÃ³n)** - Â¿Los contextos recuperados son precisos?
4. **Context Recall (Exhaustividad)** - Â¿Se recuperÃ³ todo el contexto necesario?

---

## ğŸš€ Uso RÃ¡pido

### 1. EvaluaciÃ³n BÃ¡sica (Sin Ground Truth)
```bash
python evaluate_with_ragas.py
```
- Usa 5 preguntas de ejemplo predefinidas
- Solo evalÃºa Faithfulness y Answer Relevance
- Genera respuestas automÃ¡ticamente con tu RAG

### 2. EvaluaciÃ³n con Archivo de Preguntas
```bash
python evaluate_with_ragas.py --questions test_questions_simple.json
```

### 3. EvaluaciÃ³n Completa (Con Ground Truth)
```bash
python evaluate_with_ragas.py --questions test_questions_with_ground_truth.json --with-ground-truth
```
- EvalÃºa las 4 mÃ©tricas principales
- Requiere archivo con respuestas correctas

### 4. Evaluar Materia EspecÃ­fica
```bash
python evaluate_with_ragas.py --materia "Probabilidad y estadÃ­stica"
```

### 5. Personalizar NÃºmero de Chunks
```bash
python evaluate_with_ragas.py -k 10
```
- Recupera 10 chunks por pregunta (default: 5)

---

## ğŸ“ Archivos de Entrada

### Formato JSON de Preguntas

**Sin ground truth:**
```json
[
    {
        "question": "Â¿QuÃ© es una variable aleatoria?"
    },
    {
        "question": "Â¿CuÃ¡l es la definiciÃ³n de esperanza matemÃ¡tica?"
    }
]
```

**Con ground truth:**
```json
[
    {
        "question": "Â¿QuÃ© es una variable aleatoria?",
        "ground_truth": "Una variable aleatoria es una funciÃ³n que asigna un valor numÃ©rico a cada resultado..."
    },
    {
        "question": "Â¿CuÃ¡l es la definiciÃ³n de esperanza matemÃ¡tica?",
        "ground_truth": "La esperanza matemÃ¡tica es el promedio ponderado..."
    }
]
```

---

## ğŸ“Š Archivos Generados

Todos los resultados se guardan en `./evaluation_results/`:

1. **`ragas_evaluation_results.json`** - Resultados detallados en JSON
2. **`ragas_evaluation_report.txt`** - Reporte legible en texto
3. **`test_data.json`** - Preguntas, respuestas y contextos usados

---

## ğŸ“ˆ InterpretaciÃ³n de MÃ©tricas

### Faithfulness (Fidelidad): 0.0 - 1.0
- **1.0** = La respuesta estÃ¡ completamente basada en el contexto
- **0.8-0.9** = Muy buena fidelidad
- **0.6-0.8** = Fidelidad aceptable
- **< 0.6** = La respuesta incluye informaciÃ³n no presente en el contexto

**Â¿QuÃ© mide?** Si el sistema "inventa" informaciÃ³n o se mantiene fiel al contexto recuperado.

### Answer Relevance (Relevancia): 0.0 - 1.0
- **1.0** = La respuesta es perfectamente relevante a la pregunta
- **0.8-0.9** = Muy relevante
- **0.6-0.8** = Relevancia aceptable
- **< 0.6** = La respuesta no responde bien la pregunta

**Â¿QuÃ© mide?** Si el sistema responde exactamente lo que se preguntÃ³.

### Context Precision (PrecisiÃ³n): 0.0 - 1.0
âš ï¸ Requiere ground_truth

- **1.0** = Todos los chunks recuperados son relevantes
- **0.8-0.9** = La mayorÃ­a de chunks son relevantes
- **0.6-0.8** = Algunos chunks irrelevantes
- **< 0.6** = Muchos chunks no relevantes

**Â¿QuÃ© mide?** La calidad del retrieval - si traes chunks Ãºtiles o "ruido".

### Context Recall (Exhaustividad): 0.0 - 1.0
âš ï¸ Requiere ground_truth

- **1.0** = Se recuperÃ³ toda la informaciÃ³n necesaria
- **0.8-0.9** = Se recuperÃ³ casi toda la informaciÃ³n
- **0.6-0.8** = Falta informaciÃ³n relevante
- **< 0.6** = Falta mucha informaciÃ³n necesaria

**Â¿QuÃ© mide?** Si el retrieval encuentra TODO lo necesario para responder.

---

## ğŸ’¡ Ejemplos Completos

### Ejemplo 1: EvaluaciÃ³n RÃ¡pida
```bash
# Evaluar con preguntas predefinidas
python evaluate_with_ragas.py

# Salida esperada:
# âœ“ Faithfulness: 0.85 (BUENO)
# âœ“ Answer Relevance: 0.78 (BUENO)
```

### Ejemplo 2: EvaluaciÃ³n Completa
```bash
# Usar archivo con ground truths
python evaluate_with_ragas.py \
    --questions test_questions_with_ground_truth.json \
    --with-ground-truth

# Salida esperada:
# âœ“ Faithfulness: 0.85
# âœ“ Answer Relevance: 0.78
# âœ“ Context Precision: 0.82
# âœ“ Context Recall: 0.75
```

### Ejemplo 3: Evaluar Solo Probabilidad
```bash
python evaluate_with_ragas.py \
    --materia "Probabilidad y estadÃ­stica" \
    --questions test_questions_with_ground_truth.json \
    --with-ground-truth \
    -k 10
```

---

## ğŸ”§ Opciones Avanzadas

```bash
python evaluate_with_ragas.py \
    --questions <archivo.json>        # Archivo con preguntas
    --with-ground-truth                # Usar ground_truths del archivo
    --materia <nombre>                 # Filtrar por materia
    -k <nÃºmero>                        # Chunks a recuperar (default: 5)
    --output-dir <directorio>          # DÃ³nde guardar resultados
```

---

## âš™ï¸ QuÃ© Hace el Script Internamente

1. **Inicializa el Pipeline RAG**
   - Conecta a ChromaDB
   - Verifica que hay documentos cargados

2. **Para cada pregunta:**
   - Recupera K chunks relevantes del vector store
   - Genera una respuesta usando el LLM + contexto
   - Guarda pregunta, respuesta y contextos

3. **EvalÃºa con RAGAS:**
   - Calcula mÃ©tricas segÃºn datos disponibles
   - Genera estadÃ­sticas (promedio, desviaciÃ³n, min, max)

4. **Guarda Resultados:**
   - JSON con resultados detallados
   - Reporte en texto plano
   - Datos de prueba usados

---

## ğŸ¯ Valores Objetivo

Para un buen sistema RAG acadÃ©mico:

| MÃ©trica | Objetivo MÃ­nimo | Objetivo Ideal |
|---------|----------------|----------------|
| Faithfulness | > 0.75 | > 0.85 |
| Answer Relevance | > 0.70 | > 0.80 |
| Context Precision | > 0.70 | > 0.85 |
| Context Recall | > 0.65 | > 0.80 |

---

## ğŸ› Troubleshooting

### Error: "No documents in database"
**SoluciÃ³n:**
```bash
python initialize_chroma.py
```

### Error: "ground_truth required"
**Problema:** Intentas evaluar Context Precision/Recall sin ground_truths

**SoluciÃ³n:**
- Agrega ground_truths a tu archivo JSON, O
- No uses `--with-ground-truth` (solo evaluarÃ¡ Faithfulness y Relevance)

### MÃ©tricas muy bajas (< 0.5)
**Posibles causas:**
- Chunks muy pequeÃ±os (aumenta CHUNK_SIZE)
- Estrategia de chunking inadecuada
- Embeddings de baja calidad
- K muy bajo (aumenta -k)

**Soluciones:**
1. Aumentar CHUNK_SIZE en .env
2. Probar estrategia 'semantic' o 'latex'
3. Aumentar K a 10-15 chunks
4. Verificar calidad de documentos fuente

### EvaluaciÃ³n muy lenta
**Normal:** RAGAS usa LLM para evaluar, puede tomar tiempo

**Para acelerar:**
- Reduce nÃºmero de preguntas
- Reduce K (menos chunks por pregunta)
- Usa menos mÃ©tricas

---

## ğŸ“š Archivos de Ejemplo Incluidos

1. **`test_questions_simple.json`**
   - 5 preguntas bÃ¡sicas sin ground_truth
   - Para evaluaciÃ³n rÃ¡pida

2. **`test_questions_with_ground_truth.json`**
   - 8 preguntas con respuestas correctas
   - Para evaluaciÃ³n completa

---

## ğŸ“ Mejores PrÃ¡cticas

### 1. Empezar Simple
```bash
# Primera evaluaciÃ³n
python evaluate_with_ragas.py
```

### 2. Iterar con Ground Truth
```bash
# Crear archivo con tus preguntas y respuestas correctas
python evaluate_with_ragas.py \
    --questions mis_preguntas.json \
    --with-ground-truth
```

### 3. Optimizar ParÃ¡metros
- Si Context Precision es baja â†’ Mejorar calidad de embeddings/chunking
- Si Context Recall es baja â†’ Aumentar K
- Si Faithfulness es baja â†’ Mejorar prompt del LLM
- Si Answer Relevance es baja â†’ Verificar calidad de retrieval

### 4. Evaluar Regularmente
- DespuÃ©s de cambiar CHUNK_SIZE
- DespuÃ©s de cambiar estrategia de chunking
- DespuÃ©s de agregar nuevos documentos
- DespuÃ©s de cambiar modelo de embeddings

---

## ğŸ“ Crear tus Propias Preguntas de Prueba

### Template BÃ¡sico:
```json
[
    {
        "question": "Tu pregunta aquÃ­",
        "ground_truth": "Respuesta correcta esperada"
    }
]
```

### Tips:
- Usa preguntas representativas de tu dominio
- Incluye diferentes niveles de dificultad
- Cubre diferentes temas/materias
- Las respuestas ground_truth deben ser precisas
- MÃ­nimo 10-20 preguntas para evaluaciÃ³n robusta

---

## ğŸš€ PrÃ³ximos Pasos

1. **Ejecuta tu primera evaluaciÃ³n:**
   ```bash
   python evaluate_with_ragas.py
   ```

2. **Revisa los resultados** en `./evaluation_results/`

3. **Analiza las mÃ©tricas** y identifica Ã¡reas de mejora

4. **Optimiza tu sistema** basÃ¡ndote en los resultados

5. **Re-evalÃºa** despuÃ©s de cada cambio

---

Â¿Necesitas ayuda? Revisa los ejemplos o ejecuta:
```bash
python evaluate_with_ragas.py --help
```

